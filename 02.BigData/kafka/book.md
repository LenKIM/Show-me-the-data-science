# 카프카 디자인

링크드인에서 처음에 ActiveMQ를 사용했지만, 수많은 트래픽과 데이터를 처리하기에는 한계가 있었을 뿐만 아니라 운영 관리조차 어려웠더. 이에 링크드인에서 카프카를 설계함.



### 1. 분산 시스템

분산 시스템은 네트워크로 이루어진 컴퓨터들의 그룹으로서 시스템 전체가 공통의 목표를 가진다. 같은 역할을 하는 여러 대의 서버로 이뤄진 서버그룹은 분산 시스템이라고 한다. 

		- 단일 시스템보다 더 높은 성능을 얻을 수있다.
		- 분산 시스템 중 하나의 서버 또는 노드 등이 장애가 발생하면 다른 서버 또는 노드가 대신 처리한다.
		- 시스템 확장이 용이.



### 2. 페이지 캐시

카프카는 처리량을 높이기 위한 기능을 몇 가지 추가했고 그 기능 중 하나가 페이지 캐시를 이용하는 것.

OS는 물리적 메모리에 애플리케이션이 사용하는 부분을 할당하고 남은 잔여 메모리 일부를 페이지 캐시로 유지해 OS의 전체적인 성능 향상을 높이게 됩니다. 

### 3. 배치 전송 처리

 서버와 클라이언트 사이 또는 서버 내부적으로 데이터를 주고받는 과정에서는 I/O가 발생. 빈번하게 일어날경우 속도가 저하될 수 있다.

![](https://ws4.sinaimg.cn/large/006tNc79gy1g2a02h648aj30vv0u0b2a.jpg)

## 카프카 데이터 모델

카프카가가 고성능, 고가용성 메시징 애플리케이션으로 발전한 데는 토픽과 파이션이라는 데이터 모델이 큰 역할을 했다.

### 1. 토픽의 이해

카프카 클러스터는 Topic이라 불리는 곳에 데이터를 저장한다. 우리가 많이 사용하는 메일 시스템과 비교하면, 토픽은 메일주소라고 생각하면 쉽다.

카프카에서는 데이터를 구분하기 위한 단위로 토픽이라는 용어를 사용하는데, 249자 미만으로 영문,숫자, '-','.','_','-'를 조합하여 자유롭게 만들 수 있다.

![](https://ws4.sinaimg.cn/large/006tNc79gy1g2a08c3o2mj313y0u01ky.jpg)

### 2. 파티션의 이해

파티션이란 토픽을 분할한 것이다. 그럼 왜 하나의 토픽을 파티셔닝하는 걸까?



4개의 메시지를 한개의 파티션으로 보내면 4초

여기에서는 소요시간계산이 용이하도록 메시지를 보내는 데 걸리는 시간은 1초라고 가정하고, 프로튜서만 4개로 변경하고 파티션은 1개로 유지하면, 4배의 성능을 보장할 수 있지만, 이렇게 되면 큐시스템의 제약조건으로 메시지의 순서를 보장할 수 없다.

결국 카푸카에서 효율적인 메시지 전송과 속도를 높이려면 토픽의 파티션 수를 늘려줘야 한다. 뉴스토픽에 대해 파티션 수를 1개에서 4개로 변경하고, 파티션 수와 동일하게 프로튜서 수도 4개로 늘림. 각 프로듀서는 하나의 메시지를 뉴스토픽의 파티션으로 보내게 된다. 빠른 전송을 위해서는 토픽의 파티션을 눌려줘야 하며, 그 수만큼 프로튜서 수도 늘려야 제대로 된 효과를 볼 수 있다.



**그럼 반대로 무조건 파티션의 수가 늘려야하나??**
발생할 수 있는 문제점은 무엇?

- 파일 핸들러의 낭비
- 장애 복구 시간 증가

**그렇다면 내 토픽의 적절한 파티션 수는??**

일단 원하는 목표 처리량의 기준을 잡아야 한다.

프로튜서 입장에서 4개의 프로튜서를 통해 각각 초당 10개의 메시지를 카프카의 토픽으로 보낸다면, 카프카의 토픽에서 초당 40개의 메시지를 받아줘야 합니다. 만약 해당 토픽에서 파티션을 1로 했을 때 초당 10개의 메시지를 받아준다면 파티션을 4로 늘려서 목표 처리량을 처리할 수 있도록 변경합니다. 토픽의 파티션 수를 4개로 정해 프로튜서의 목표치는 달성 했다.

하지만 카프카에서는 컨슈머도 있기 때문에 컨슈머의 입장도 고려해야 합니다. 컨슈머 입장에서 8개의 컨슈머를 통해 각각 초당 5개의 메시지를 카프카의 토픽에서 가져올 수 있다면, 해당 토픽의 파티션 수는 컨슈머 수와 동일하게 8개로 맞추어 켠슈머마다 각각의 파티션에 접근할 수 있게 해야 합니다.

### 3. 오프셋과 메시지 순서

카프카에서는 각 파티션마다 메시지가 저장되는 위치를 오프셋이라고 부르고 오프셋은 파티션 내에서 유일하고 순차적으로 증가하는 숫자(64비트) 형태로 되어 있다.



## 카프카의 고가용성과 리플리케이션

분산 애플리케이션으로 서버의 물리적 장애가 발생하는 경우에도 높은 가용성을 보장. 이를 위해 카프카는 리플리케이션(Replication) 기능을 제공합니다. 카프카의 리플리케이션은 토픽 자체를 리플리케이션하는 것이 아니라,

 **토픽을 이루는 각각의 파티션을 리플리케이션하는 것.**

- 리플리케이션 팩터와 리더, 팔로워의 역할

  카프카에는 리플리케이션 팩터(Factor)라는 것이 있음. 카프카의 기본값은 1로, 변경하고 싶다면

  레플리는 2로 설정해주는 것이 좋다.

  리더와 팔로워는 각자 역할이 나뉘어 있는데 가장 중요한 핵심은 모든 읽기와 쓰기가 리더를 통해서만 일어난다는 점, 즉 팔로워는 리더의 데이터를 그대로 리플리케이션만 하고 읽기와 쓰기에는 관여하지 않습니다. 리더와 팔로워는 저장된 데이터의 순서도 일치하고 동일한 오프셋과 메시지를 갖게 됩니다.


  단점은,  100G이면, 리플리케이션을 하는 다른 브로커에도 100G 들어간다.

- 리더와 팔로워의 관리

  리더는 모든 데이터의 읽기 쓰기에 대한 요청에 응답하면서 데이터를 저장해나가고, 팔로워는 리더를 주기적으로 보면서 자신에게 없는 데이터를 리더로부터 주기적으로 가져오는 방법으로 리플리케이션을 유지합니다. 리더와 팔로워 모두 주어진 역할에 맞게 잘 동작하고 있다면 전혀 문제가 없지만 팔로워에 문제가 있어 리더로부터 데이터를 가져오지 못하면서 정합성이 맞지 않게 된다면 어떻게 될까요? 결국 리더가 다운되는 경우 팔로워가 새로운 리더로 승격되어야 하는데, 데이터가 읽치하지 않으므로 큰 문제가 발생할 수 있다. 카프카에서는 이러한 현상을 방지하고자 ISR(In Sync Replica)라는 개념을 도입했다.

  **ISR** - 현재 리플리케이션되고 있는 리플리케이션 그룹. ISR에는 중요한 규칙 하나가 있는데, 그 규칙은 ISR에 속해있는 구성원만이 리더의 자격을 가질 수 있다. 
  peter토픽이 리플리케이션 팩터2로 구성되어 리더는 1번 브로커, 팔로워는 2번 브로커에 위치하고 있다면, ISR구성원은 1,2입니다. 그런데 급작스러운 이유로 브로커 1이 다운되면, ISR의 구성원인 2번 브로커에 있는 팔로워가 새로운 리더로 승격.

## 모든 브로커가 다운된다면

1. 마지막 리더가 살아나기를 기다린다.
2. ISR에서 추방되었지만 자동으로 리더가 된다.

0.11.0.0 이하 버전에서는 기본값으로 2번 방안 / 그 이상 버전부터는 1번 방안을 고려한다.

## 카프카에서 사용하는 주키퍼 지노드 역할

주키퍼는 서버 여러 대를 앙상블로 구성하고, 분산 애플리케이션들이 각각 클라이언트가 되어 주키퍼 서버들과 커넥션을 맺은 후 상태 정보등을 주고받게 됩니다. 상태 정보들은 주키퍼의 znode라 불리는 곳에 키-값 형태로 저장하고, 지노드에 키-값이 저장된 것을 이용하여 분산 애플리케이션들은 서로 데이터를 주고받게 도비니다. 주키퍼에서 사용되는 지노드는 데이터를 저장하기 위한 공간 이름을 말하는 것으로,일반 컴퓨터의 파일이나 폴더 개념이라고 생각하면 됩니다.



**그럼 카프라에서 지노드의 역할은?**



만약 지노드를 확인하고 싶다면, 주키퍼 서버에 접속한 후 zkCli.sh를 실행

`/usr/local/zookeeper/bin/zkCli.sh`

접속한 후 리스트를 확인하는  ls / 명령어를 이용하여 지노드 확인

