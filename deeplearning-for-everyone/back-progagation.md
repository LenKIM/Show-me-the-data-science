### back-progagation이란?

다층 퍼셉트론에서의 최적화 과정을 오차 역전파(back progation)



1. 임의의 초기 가중치(w(1))를 준 뒤 결과(y(out))를 계산한다.
2. 계산 결과와 우리가 원하는 값 사이의 오차를 구한다.
3. 경사 하강법을 이용해 바로 앞 가중치를 오차가 작아지는 방향으로 업데이트한다.
4. 1~3과정을 더이상 오차가 줄어들지 않을 때까지 반복한다.



$W(t+1)$= $W_t$ - $aW/a오차$ 

새가중치는 = 현 가중치에서 '가중치에 대한 기울기'를 뺀 값.



1. 환경 변수 지정  
   : 환경 변수에는 입력 값과 타킷 결괏값이 포함된 데이터셋, 학습률 등이 포함/ 활성화 함수와 가중치 등도 선언
2. 신경망 실행  
   : 초깃값을 입력하여 활성화 함수와 가중치를 거쳐 결괏값이 나오게 합니다.
3. 결과를 실제 값과 비교  
   : 오차를 측정합니다.
   1. 오차가 많이 발생한다면, 출력층 가중치 수정
   2. 은닉층 가중치 수정
4. 역전파 실행  
   : 출력층과 은닉층의 가중치를 수정합니다.
5. 결과 출력

---



### 이제 신경망에서 딥러닝으로...

다층 퍼셉트론이 오차 역전파를 만나 신경망이 되었고, 신경망이 XOR문제를 가볍게 해결했다. 따라서 이제 신경망을 차곡차곡 쌓아올리면 마치 사람처럼 생각하고 판단하는 인공지능이 될거라 예측했지만...?



![](https://ws3.sinaimg.cn/large/006tNbRwgy1fwdb8kddf5j318a0h80uy.jpg)

층이 많아지면 결과가 좋을줄 알았지만 Nope!



### 왜일까? 왜 좋지 못했을까?

오차 역전파는 출력층으로부터 하나씩 앞으로 되돌아가며 각 층의 가중치를 수정하는 방법입니다. 가중치를 수정하려면 미분 값, 즉 기울기가 필요하다고 배웠습니다. 그런데 층이 늘어나면서 기울기가 중간에 0이 되어버리는 기울기 소실(vanishing gradient) 문제가 발생하기 시작했습니다.

이는 활성화 함수로 사용된 시그모이드 함수의 특징 때문입니다.

미분하게 되도 최대값이 0.3 곱하면 곱할 수록 점점 작아지기 때문에 다른 함수들이 발생하게 되었다.

대표적으로 LeRu함수를 말할 수 있다.

**그럼 학습을 시키는 방법을 알았다면, 이동을 어떻게 할것인가에 대해 궁금증을 가질 수 있다.**

가중치를 업데이트하는 방법으로 우리는 경사 하강법을 배웠지만, 한번 업데이트 할 때마다 전체 데이터를 미분해야 하므로 계산량이 많다는 단점이 있다. 이러한 점을 보완한 고급 경사 하강법이 등장하면서 딥러닝의 발전 속도는 더욱 빨라졌다.





이걸 계산하는 방식에 대한 이해가 필요하다.

------

**어떻게 계산하는 걸까??**

//TODO 계산 방식 넣기