# 15장. 오토인코더

`https://github.com/rickiepark/handson-ml`

어떤 감독 없이도(즉, 레이블되어 있지 않은 훈련 데이터를 사용해서) 입력 데이터의 효율적인 표현인 coding을 학습할 수 있는 인공 신경망.

- 입력보다 휠씬 낮은 차원을 가지므로 오토인코더가 차원 축소에 유용하게 사용됨.
- 오토인코더가 강력한 특성 추출기처럼 작동하기 때문에 심층신경망의 비지도 사전훈련에 사용될 수 있다는 것. 또한 훈련 데이터와 매우 비슷한 새로운 데이터를 생성하는 이를 **Generative model** 
- 오토인코더가 학습하는 것은 단순히 입력을 출력으로 복사하는 것. 그러나 이 과정에서 네트워크에 여러가지 제약을 가해 오히러 어려운 작업을 만듬.

*즉, 차원 축소, 특성 추출, 비지도 사전훈련, 생성 모델을 위한 오토인코더가 어떻게 작동하는지, 어떤 종류의 제약조건을 가할 수 있는지, 텐서플로를 사용해 어떻게 구현할 수 있는 지 살펴보기.*

### 15.1 효율적인 데이터 표현

- 40,27,25,36,81,57,10,73,19,68
- 50,25,76,38,19,58,29,88,44,22,11,34,17,52,26,13,40,20

첫번째가 더 쉬워보이지만, 2번째에 자세히 살펴보면 헤일스톤 수열로서 규칙만 알만 더 쉬움.

긴 수열을 기억하기 어렵기 때문에 패턴을 찾는 것이 유용하며, 이와 마찬가지 이유로 훈련하는 동안 오토인코더에 제약을 가해서 데이터에 있는 패턴을 찾아 활용.

오토인코더는 항상 두 부분으로 나뉨

- 입력을 내부 표현으로 바꾸는 encoder(또는 인지 네트워크(recognition network))
- 내부 표현을 출력으로 바꾸는 decoder(또는 생성 네트워크(generative network))

![](https://ws1.sinaimg.cn/large/006tNc79gy1fzkzveqf3mj30xa0idabd.jpg)

다층 퍼셉트론과 동일한 구조를 가지지만, 오토인코더가 입력을 재구성하기 때문에 출력을 종종 **재구성(reconstruction)** 이라고 부름. 비용 함수는 재구성이 입력과 다를 때 모델에 벌점을 부과하는 **재구성 손실**을 포함.

### 15.2 과소완전 선형 오토인코더로 PCA 수행하기

***과소완전***

- 내부 표현이 입력데이터보다 저차원(3차원 대신 2차원)
- 입력을 코딩으로 간단히 복사할 수 없으며, 입력과 똑같은 것을 출력하기 위한 다른 방법을 찾아야 함. 이는 데이터에서 가장 중요한 특성을 학습하도록 만듬.

```python
import tensorflow as tf

reset_graph()

n_inputs = 3
n_hidden = 2  # 코딩 유닛
n_outputs = n_inputs

learning_rate = 0.01

X = tf.placeholder(tf.float32, shape=[None, n_inputs])
hidden = tf.layers.dense(X, n_hidden)
outputs = tf.layers.dense(hidden, n_outputs)

reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))

optimizer = tf.train.AdamOptimizer(learning_rate)
training_op = optimizer.minimize(reconstruction_loss)

init = tf.global_variables_initializer()
```

- 출력의 개수가 입력의 개수와 동일합니다.
- 단순한 PCA를 수행하기 위해서는 활성화 함수를 사용하지 않으며(즉, 모든 뉴런이 선형입니다.) 비용함수는 MSE입니다.

```python
n_iterations = 1000
codings = hidden

with tf.Session() as sess:
    init.run()
    for iteration in range(n_iterations):
        training_op.run(feed_dict={X: X_train})
    codings_val = codings.eval(feed_dict={X: X_test})
fig = plt.figure(figsize=(4,3))
plt.plot(codings_val[:,0], codings_val[:, 1], "b.")
plt.xlabel("$z_1$", fontsize=18)
plt.ylabel("$z_2$", fontsize=18, rotation=0)
save_fig("linear_autoencoder_pca_plot")
plt.show()
```

![](https://ws3.sinaimg.cn/large/006tNc79gy1fzl0a77vb1j307s05saa6.jpg)

### 15.3 적층 오토인코더

여러 개의 은닉층을 가질 수 있음.

이런 경우를 **적층 오토인코더(Stacked autoencoder)** 또는 **심층 오토인코더라(deep autoencoder)**라고 함.

*적층 오토인코더는 전형적으로 가운데 은닉층(코딩층)을 기준으로 대칭*

![](https://ws3.sinaimg.cn/large/006tNc79gy1fzl0gqq9wcj30xc0gs416.jpg)

구현은.

```python
from functools import partial

n_inputs = 28 * 28
n_hidden1 = 300
n_hidden2 = 150  # 코딩 유닛
n_hidden3 = n_hidden1
n_outputs = n_inputs

learning_rate = 0.01
l2_reg = 0.0001

X = tf.placeholder(tf.float32, shape=[None, n_inputs])

he_init = tf.variance_scaling_initializer() # He 초기화
#아래와 동일합니다:
#he_init = lambda shape, dtype=tf.float32: tf.truncated_normal(shape, 0., stddev=np.sqrt(2/shape[0]))
l2_regularizer = tf.contrib.layers.l2_regularizer(l2_reg)
my_dense_layer = partial(tf.layers.dense,
                         activation=tf.nn.elu,
                         kernel_initializer=he_init,
                         kernel_regularizer=l2_regularizer)

hidden1 = my_dense_layer(X, n_hidden1)
hidden2 = my_dense_layer(hidden1, n_hidden2)
hidden3 = my_dense_layer(hidden2, n_hidden3)
outputs = my_dense_layer(hidden3, n_outputs, activation=None)

reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))

reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
loss = tf.add_n([reconstruction_loss] + reg_losses)

optimizer = tf.train.AdamOptimizer(learning_rate)
training_op = optimizer.minimize(loss)

init = tf.global_variables_initializer()
saver = tf.train.Saver() # 책에는 없음
```

```python
n_epochs = 5
batch_size = 150

with tf.Session() as sess:
    init.run()
    for epoch in range(n_epochs):
        n_batches = len(X_train) // batch_size
        for iteration in range(n_batches):
            print("\r{}%".format(100 * iteration // n_batches), end="") # 책에는 없음
            sys.stdout.flush()                                          # 책에는 없음
            X_batch, y_batch = next(shuffle_batch(X_train, y_train, batch_size))
            sess.run(training_op, feed_dict={X: X_batch})
        loss_train = reconstruction_loss.eval(feed_dict={X: X_batch})   # 책에는 없음
        print("\r{}".format(epoch), "훈련 MSE:", loss_train)             # 책에는 없음
        saver.save(sess, "./my_model_all_layers.ckpt")                  # 책에는 없음

0 훈련 MSE: 0.022985728
1 훈련 MSE: 0.0114945555
2 훈련 MSE: 0.011794798
3 훈련 MSE: 0.01163566
4 훈련 MSE: 0.0114042135
```

```python
def show_reconstructed_digits(X, outputs, model_path = None, n_test_digits = 2):
    with tf.Session() as sess:
        if model_path:
            saver.restore(sess, model_path)
#         X_test = mnist.test.images[:n_test_digits]
        outputs_val = outputs.eval(feed_dict={X: X_test[:n_test_digits]})

    fig = plt.figure(figsize=(8, 3 * n_test_digits))
    for digit_index in range(n_test_digits):
        plt.subplot(n_test_digits, 2, digit_index * 2 + 1)
        plot_image(X_test[digit_index])
        plt.subplot(n_test_digits, 2, digit_index * 2 + 2)
        plot_image(outputs_val[digit_index])
        
show_reconstructed_digits(X, outputs, "./my_model_all_layers.ckpt")
save_fig("reconstruction_plot")
```

![](https://ws3.sinaimg.cn/large/006tNc79gy1fzl0mgmfxxj30df0bsjrs.jpg)

#### 15.3.2 가중치 묶기

```python
n_inputs = 28 * 28
n_hidden1 = 300
n_hidden2 = 150  # 코딩 유닛
n_hidden3 = n_hidden1
n_outputs = n_inputs

learning_rate = 0.01
l2_reg = 0.0005

activation = tf.nn.elu
regularizer = tf.contrib.layers.l2_regularizer(l2_reg)
initializer = tf.variance_scaling_initializer()

X = tf.placeholder(tf.float32, shape=[None, n_inputs])

weights1_init = initializer([n_inputs, n_hidden1])
weights2_init = initializer([n_hidden1, n_hidden2])

weights1 = tf.Variable(weights1_init, dtype=tf.float32, name="weights1")
weights2 = tf.Variable(weights2_init, dtype=tf.float32, name="weights2")
weights3 = tf.transpose(weights2, name="weights3")  # 가중치 묶기
weights4 = tf.transpose(weights1, name="weights4")  # 가중치 묶기

biases1 = tf.Variable(tf.zeros(n_hidden1), name="biases1")
biases2 = tf.Variable(tf.zeros(n_hidden2), name="biases2")
biases3 = tf.Variable(tf.zeros(n_hidden3), name="biases3")
biases4 = tf.Variable(tf.zeros(n_outputs), name="biases4")

hidden1 = activation(tf.matmul(X, weights1) + biases1)
hidden2 = activation(tf.matmul(hidden1, weights2) + biases2)
hidden3 = activation(tf.matmul(hidden2, weights3) + biases3)
outputs = tf.matmul(hidden3, weights4) + biases4

reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))
reg_loss = regularizer(weights1) + regularizer(weights2)
loss = reconstruction_loss + reg_loss

optimizer = tf.train.AdamOptimizer(learning_rate)
training_op = optimizer.minimize(loss)

init = tf.global_variables_initializer()

n_epochs = 5
batch_size = 150

with tf.Session() as sess:
    init.run()
    for epoch in range(n_epochs):
        n_batches = len(X_train) // batch_size
        for iteration in range(n_batches):
            print("\r{}%".format(100 * iteration // n_batches), end="")
            sys.stdout.flush()
            X_batch, y_batch = next(shuffle_batch(X_train, y_train, batch_size))
            sess.run(training_op, feed_dict={X: X_batch})
        loss_train = reconstruction_loss.eval(feed_dict={X: X_batch})
        print("\r{}".format(epoch), "훈련 MSE:", loss_train)
        saver.save(sess, "./my_model_tying_weights.ckpt")
```

`weights_decoder = tf.transpose(weights_encoder)`

**이 부분이 주목할 만한 부분**

- weight3와 weight4는 변수로 선언되지 않았고 각각 weight2와 weight1의 전치
- 변수가 아니기 때문에 규제에 사용되지 않습니다. weights1과 wegight2
- 편향은 묶지도 않고고 규제도 하지 않음

#### 15.3.3 한 번에 한 층씩 훈련하기.

![](https://ws1.sinaimg.cn/large/006tNc79gy1fzl0vp0w2dj30xh0i1tdg.jpg)

```python
from functools import partial

def train_autoencoder(X_train, n_neurons, n_epochs, batch_size,
                      learning_rate = 0.01, l2_reg = 0.0005, seed=42,
                      hidden_activation=tf.nn.elu,
                      output_activation=tf.nn.elu):
    graph = tf.Graph()
    with graph.as_default():
        tf.set_random_seed(seed)

        n_inputs = X_train.shape[1]

        X = tf.placeholder(tf.float32, shape=[None, n_inputs])
        
        my_dense_layer = partial(
            tf.layers.dense,
            kernel_initializer=tf.variance_scaling_initializer(),
            kernel_regularizer=tf.contrib.layers.l2_regularizer(l2_reg))

        hidden = my_dense_layer(X, n_neurons, activation=hidden_activation, name="hidden")
        outputs = my_dense_layer(hidden, n_inputs, activation=output_activation, name="outputs")

        reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))

        reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
        loss = tf.add_n([reconstruction_loss] + reg_losses)

        optimizer = tf.train.AdamOptimizer(learning_rate)
        training_op = optimizer.minimize(loss)

        init = tf.global_variables_initializer()

    with tf.Session(graph=graph) as sess:
        init.run()
        for epoch in range(n_epochs):
            n_batches = len(X_train) // batch_size
            for iteration in range(n_batches):
                print("\r{}%".format(100 * iteration // n_batches), end="")
                sys.stdout.flush()
                indices = rnd.permutation(len(X_train))[:batch_size]
                X_batch = X_train[indices]
                sess.run(training_op, feed_dict={X: X_batch})
            loss_train = reconstruction_loss.eval(feed_dict={X: X_batch})
            print("\r{}".format(epoch), "훈련 MSE:", loss_train)
        params = dict([(var.name, var.eval()) for var in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)])
        hidden_val = hidden.eval(feed_dict={X: X_train})
        return hidden_val, params["hidden/kernel:0"], params["hidden/bias:0"], params["outputs/kernel:0"], params["outputs/bias:0"]
```

```python
hidden_output, W1, b1, W4, b4 = train_autoencoder(X_train, n_neurons=300, n_epochs=4, batch_size=150,
                                                  output_activation=None)
_, W2, b2, W3, b3 = train_autoencoder(hidden_output, n_neurons=150, n_epochs=4, batch_size=150)
```

#### 재구성 시각화 / 특성 시각화

### 15.4 적층 오토인코더를 사용한 비지도 사전훈련

일반적인 MNIST + CNN

```python
reset_graph()

n_inputs = 28 * 28
n_hidden1 = 300
n_hidden2 = 150
n_outputs = 10

learning_rate = 0.01
l2_reg = 0.0005

activation = tf.nn.elu
regularizer = tf.contrib.layers.l2_regularizer(l2_reg)
initializer = tf.variance_scaling_initializer()

X = tf.placeholder(tf.float32, shape=[None, n_inputs])
y = tf.placeholder(tf.int32, shape=[None])

weights1_init = initializer([n_inputs, n_hidden1])
weights2_init = initializer([n_hidden1, n_hidden2])
weights3_init = initializer([n_hidden2, n_outputs])

weights1 = tf.Variable(weights1_init, dtype=tf.float32, name="weights1")
weights2 = tf.Variable(weights2_init, dtype=tf.float32, name="weights2")
weights3 = tf.Variable(weights3_init, dtype=tf.float32, name="weights3")

biases1 = tf.Variable(tf.zeros(n_hidden1), name="biases1")
biases2 = tf.Variable(tf.zeros(n_hidden2), name="biases2")
biases3 = tf.Variable(tf.zeros(n_outputs), name="biases3")

hidden1 = activation(tf.matmul(X, weights1) + biases1)
hidden2 = activation(tf.matmul(hidden1, weights2) + biases2)
logits = tf.matmul(hidden2, weights3) + biases3

cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)
reg_loss = regularizer(weights1) + regularizer(weights2) + regularizer(weights3)
loss = cross_entropy + reg_loss
optimizer = tf.train.AdamOptimizer(learning_rate)
training_op = optimizer.minimize(loss)

correct = tf.nn.in_top_k(logits, y, 1)
accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))

init = tf.global_variables_initializer()
pretrain_saver = tf.train.Saver([weights1, weights2, biases1, biases2])
saver = tf.train.Saver()
```

```python
n_epochs = 4
batch_size = 150
n_labeled_instances = 20000

with tf.Session() as sess:
    init.run()
    for epoch in range(n_epochs):
        n_batches = n_labeled_instances // batch_size
        for iteration in range(n_batches):
            print("\r{}%".format(100 * iteration // n_batches), end="")
            sys.stdout.flush()
            indices = rnd.permutation(n_labeled_instances)[:batch_size]
            X_batch, y_batch = X_train[indices], y_train[indices]
            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})
        accuracy_val = accuracy.eval(feed_dict={X: X_batch, y: y_batch})
        print("\r{}".format(epoch), "검증 세트 정확도:", accuracy_val, end=" ")
        saver.save(sess, "./my_model_supervised.ckpt")
        test_val = accuracy.eval(feed_dict={X: X_test, y: y_test})
        print("테스트 정확도:", test_val)
```

사전훈련없이 평범하게 훈련시켰을 경우.

```
0 검증 세트 정확도: 0.93333334 테스트 정확도: 0.9191
1 검증 세트 정확도: 0.97333336 테스트 정확도: 0.9371
2 검증 세트 정확도: 0.9866667 테스트 정확도: 0.9318
3 검증 세트 정확도: 0.97333336 테스트 정확도: 0.9403
```

사전 훈련된 오토인코더의 첫 두개의 층을 재사용

여기서 `pretrain_saver` 이 부분은 사전에 캐싱된 데이터 사용함.

```python
n_epochs = 4
batch_size = 150
n_labeled_instances = 20000

#training_op = optimizer.minimize(loss, var_list=[weights3, biases3])  # layers 1와 2를 동결 (선택사항)

with tf.Session() as sess:
    init.run()
    pretrain_saver.restore(sess, "./my_model_cache_frozen.ckpt")
    for epoch in range(n_epochs):
        n_batches = n_labeled_instances // batch_size
        for iteration in range(n_batches):
            print("\r{}%".format(100 * iteration // n_batches), end="")
            sys.stdout.flush()
            indices = rnd.permutation(n_labeled_instances)[:batch_size]
            X_batch, y_batch = X_train[indices], y_train[indices]
            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})
        accuracy_val = accuracy.eval(feed_dict={X: X_batch, y: y_batch})
        print("\r{}".format(epoch), "훈련 정확도:", accuracy_val, end="\t")
        saver.save(sess, "./my_model_supervised_pretrained.ckpt")
        test_val = accuracy.eval(feed_dict={X: X_test, y: y_test})
        print("테스트 정확도:", test_val)
```

```
INFO:tensorflow:Restoring parameters from ./my_model_cache_frozen.ckpt
0 훈련 정확도: 0.9533333	테스트 정확도: 0.9195
1 훈련 정확도: 0.96	테스트 정확도: 0.9419
2 훈련 정확도: 0.97333336	테스트 정확도: 0.9378
3 훈련 정확도: 0.9866667	테스트 정확도: 0.9411
```

 모든 훈련 데이터를 사용해 오토인코더를 훈련시키고, 그다음에 인코더 층을 재사용하여 새로운 신경망을 만든다. 지금까지는 흥미로운 특성을 학습하도록 오토인코더를 강제하기 위해 코딩층의 크기를 제한하여 과소완전을 만듬. 코딩층의 크기를 입력과 같게 하거나 혹은 더 크게 하여 과대완전 오토인코더를 만들수도 있다.

### 15.5 잡음제거 오토인코더

**유용한 특성을 학습하도록 강제하는 다른 방법은 입력에 잡음을 추가하고, 노이즈가 없는 원본 입력을 복원하도록 훈련시키는 것.**

이렇게 하면 오토인코더가 단순히 입력을 출력으로 하지 복사하지 못하므로 데이터에 있는 패턴을 찾아야 한다.

**잡음은 입력에 추가된 순수한 가우시안 잡음이거나 드롭아웃처럼 무작위로 입력을 꺼서(off) 발생시킬 수도 있다.**

![](https://ws4.sinaimg.cn/large/006tNc79gy1fzl1q4zu8wj30yh0i3td5.jpg)

구현.

> 가우시안 잡음 Ver.

```python
n_inputs = 28 * 28
n_hidden1 = 300
n_hidden2 = 150  # 코딩 유닛
n_hidden3 = n_hidden1
n_outputs = n_inputs

learning_rate = 0.01
noise_level = 1.0

X = tf.placeholder(tf.float32, shape=[None, n_inputs])
X_noisy = X + noise_level * tf.random_normal(tf.shape(X))

hidden1 = tf.layers.dense(X_noisy, n_hidden1, activation=tf.nn.relu,
                          name="hidden1")
hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, # 책에는 없음
                          name="hidden2")                            # 책에는 없음
hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, # 책에는 없음
                          name="hidden3")                            # 책에는 없음
outputs = tf.layers.dense(hidden3, n_outputs, name="outputs")        # 책에는 없음

reconstruction_loss = tf.reduce_mean(tf.square(outputs - X)) # MSE
optimizer = tf.train.AdamOptimizer(learning_rate)
training_op = optimizer.minimize(reconstruction_loss)
    
init = tf.global_variables_initializer()
saver = tf.train.Saver()

n_epochs = 10
batch_size = 150

with tf.Session() as sess:
    init.run()
    for epoch in range(n_epochs):
        n_batches = len(X_train) // batch_size
        for iteration in range(n_batches):
            print("\r{}%".format(100 * iteration // n_batches), end="")
            sys.stdout.flush()
            X_batch, y_batch = next(shuffle_batch(X_train, y_train, batch_size))
            sess.run(training_op, feed_dict={X: X_batch})
        loss_train = reconstruction_loss.eval(feed_dict={X: X_batch})
        print("\r{}".format(epoch), "훈련 MSE:", loss_train)
        saver.save(sess, "./my_model_stacked_denoising_gaussian.ckpt")
        

0 훈련 MSE: 0.044244602
1 훈련 MSE: 0.038293216
2 훈련 MSE: 0.043376893
3 훈련 MSE: 0.041731298
4 훈련 MSE: 0.041203804
5 훈련 MSE: 0.040766567
6 훈련 MSE: 0.04216331
7 훈련 MSE: 0.040126044
8 훈련 MSE: 0.043438274
9 훈련 MSE: 0.042842094
```

> 드롭아웃

```python
n_inputs = 28 * 28
n_hidden1 = 300
n_hidden2 = 150  # 코딩 유닛
n_hidden3 = n_hidden1
n_outputs = n_inputs

learning_rate = 0.01
```

```python
dropout_rate = 0.3

training = tf.placeholder_with_default(False, shape=(), name='training')

X = tf.placeholder(tf.float32, shape=[None, n_inputs])
X_drop = tf.layers.dropout(X, dropout_rate, training=training)

hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu,
                          name="hidden1")
hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, # 책에는 없음
                          name="hidden2")                            # 책에는 없음
hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, # 책에는 없음
                          name="hidden3")                            # 책에는 없음
outputs = tf.layers.dense(hidden3, n_outputs, name="outputs")        # 책에는 없음

reconstruction_loss = tf.reduce_mean(tf.square(outputs - X)) # MSE
```

```python
optimizer = tf.train.AdamOptimizer(learning_rate)
training_op = optimizer.minimize(reconstruction_loss)
    
init = tf.global_variables_initializer()
saver = tf.train.Saver()

n_epochs = 10
batch_size = 150

with tf.Session() as sess:
    init.run()
    for epoch in range(n_epochs):
        n_batches = len(X_train) // batch_size
        for iteration in range(n_batches):
            print("\r{}%".format(100 * iteration // n_batches), end="")
            sys.stdout.flush()
            X_batch, y_batch = next(shuffle_batch(X_train, y_train, batch_size))
            sess.run(training_op, feed_dict={X: X_batch, training: True})
        loss_train = reconstruction_loss.eval(feed_dict={X: X_batch})
        print("\r{}".format(epoch), "훈련 MSE:", loss_train)
        saver.save(sess, "./my_model_stacked_denoising_dropout.ckpt")
        
0 훈련 MSE: 0.034992643
1 훈련 MSE: 0.027621135
2 훈련 MSE: 0.031030642
3 훈련 MSE: 0.02774949
4 훈련 MSE: 0.026966333
5 훈련 MSE: 0.026317744
6 훈련 MSE: 0.027312625
7 훈련 MSE: 0.025601607
8 훈련 MSE: 0.027471364
9 훈련 MSE: 0.027234565
```

15.6 희소 오토인코더
15.7 변이형 오토인코더
15.8 다른 오토인코더들
15.9 연습문제



### 오토인코더?

