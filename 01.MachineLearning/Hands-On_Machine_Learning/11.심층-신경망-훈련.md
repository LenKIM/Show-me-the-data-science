# 심층 신경망 훈련

더 깊은 신경망이 가져다 주는 위험성

- 첫째, 까다로운 Vanishing Gradient or expolding gradient 문제에 직면.
- 둘째, 이런 대규모 신경망에서는 훈련이 극단적으로 느려질 것
- 셋째, 수백만 개의 파라미터를 가진 모델은 훈련 세트에 과대적합될 위험이 큼

위 3개의 문제를 차차근



# 1-1 Vanishing Gradient 또는 expolding gradient

### 1-1 세이비어 글로럿과 요슈아 벤지오는 "Understanding the Difficulty of Trainning Deep Feedforward Neural networks" 에 따르면

- 당시 유행이였던 시그모이드 함수와- 가중치의 초기화 방법이 조합에 대해서 나왔는데, 초기화 방법이 바로 정규분포를 사용한 무작위 초기화, 즉, 이 활성화 함수와 초기화 방식을 사용했을 때 각 층에서 출력의 분산이 입력의 분산보다 더 크다는 것을 밝혀냄.
- 깊이가 깊어질수록 가중치가 0또는 1로 수렴한다는 사실을 깨달음. 그러므로 글로럿과 벤지오는 이 문제를 해결할 방법을 찾음.
- N(input)와 N(outputs)는 가중치를 초기화하려는 층의 입력과 출력 연결의 개수
- tf.layer.dense() 함수는 기본적으로 (균등분포로) 세이비어 초기화를 사용하는데, 이를 variance_scaling_initializer() 함수를 사용하여 다음과 같이 He 초기화 방식으로 바꿀 수 있음.

### 1-2 수렴하지 않는 활성화 함수

시그모이드함수가 실제 뉴런과 비슷해서 잘 되는 함수라고 생각할 수 있지만, 실제로 ReLU 함수가 더 잘된다. + 계산 속도도 더 빠름.

그러나 ReLU는 완벽하지 않다. **죽은 ReLU** 라고도 하는데, 왜냐하면, 훈련하는 동안 일부 뉴런이 0이외의 값을 출력하지 않는다는 의미로 죽었다고 말합니다. 어떤 경우에는, 특히 큰 학습률을 사용하면 신경만의 뉴련 절반이 죽어 있기도 합니다. 훈련 도중 뉴런의 가중치가 바뀌어 가중치 합이 음수가 되면 그다음부터 0을 출력하기 시작할 것이고, ReLU함수는 입력이 음수면 그래디언트가 0이 되기 때문에 이런 일이 생기면 뉴런이 다시 살아나기 어려울 것.

그래서 나온 것이 **LeakyReLU** 

**$LeakyReLU_a(z) =max(az,z)​$**

![](https://ws3.sinaimg.cn/large/006tNc79gy1fzc96gmyonj313g0u0dpp.jpg)



여기서 하이퍼파라미터 a(알파)가 이 함수의 '새는 leaky'정도를 결정한다.

일반적으로 0.01로 설정함. 그러나 **LeakyReLU** 함수가 ReLU함수보다 조금더 나은 성능을 내지만 a = 0.2로 하는 것이 a=0.01보다 더 나은 성능을 내는 것으로 보인다.

이 과정에서

- RReLU(randomized leaky ReLU)도 평가했으며, 이 함수도 꽤 잘 작동했으며(훈련 세트의 과대적합 위험을 줄이는) 규제의 역할을 하는 것처럼 보였다.

- 마지막으로 a가 훈련하는 동안 학습되는 PReLU(parametric leaky ReLU) 도 있음 이 함수는 대규모 이미지 데이터셋에서는 ReLU 보다 성능이 크게 앞섰지만, 소규모 데이터셋에서는 훈련 세트에 과대적합될 위험이 있다.

- 또 하나 중요한 함수는 ELU(exponential linear unit)라는 새로운 활성화 함수- 이 함수는 ReLU 변종의 성능을 앞도함.

  ![](https://ws3.sinaimg.cn/large/006tNc79gy1fzc9e8qmd1j329y03oq53.jpg)



결론은 그럼 은닉층에 어떤 활성화 함수를 써야 하나?

사람마다 다르겠지만, 일반적으로 ELU > LeakyReLU(그리고 변종들) > ReLU > tanh > 로지스틱 순. 실행 속도가 중요하다면 LeakyReLU가 ELU보다 나을 수 있음.

https://en.wikipedia.org/wiki/Activation_function

https://m.blog.naver.com/wideeyed/221017173808



### 1-3 배치 정규화

ELU(또는 다른 ReLU 변종)와 함께 He초기화를 사용하면 훈련 초기 단계에서 그래디언트 소실이나 폭주 문제를 크게 감소시킬 수 있지만, 훈련하는 동안 다시 발생하지 않으리란 보장은 없다.

이를 해결하기 위해 **배치정규화** (Batch Normalization BN)기법이 제안됨.  
: 훈련하는 동안 이전 층의 파라미터가 변함에 따라 각 층에 들어오는 입력의 분포가 변화되는 문제(**내부 공변량 변화- Internal Covariate Shift 문제**)

이 기법은 각 층에서 활성화 함수를 통과하기 전에 모델에 연산을 하나 추가합니다. 단순하게 입력 데이터의 평균을 0으로 만들고 정규화 한 다음, 각 층에서 두 개의 새로운 파라미터로 결괏값의 스케일을 조정하고 이동시킵니다. (하나는 스케일 조정을 위해, 다른 하나는 이동을 위해 필요합니다. ) 다시 말해 **이 연산으로 모델이 층마다 입력 데이터의 최적 스케일과 평균을 학습합니다.**

**입력 데이터의 평균을 0으로 만들고 정규화하려면 알고리즘은 평균과 표준편차를 추정해야 하므로, 현재 미니배치에서 입력의 평균과 표준편차를 평가합니다**. ( 그래서 **배치 정규화**)



![](https://ws2.sinaimg.cn/large/006tNc79gy1fzc9ylvqetj30vq0p4anr.jpg)

전체적으로 보면 배치 정규화된 층마다 스케일, 이동, 평균, 표준편차 네 개의 파라미터가 학습된다.

이 기법으로 로지스틱 활성화 함수 같이 수렴되는 활성화 함수를 사용하더라도 그래디언트 소실 문제가 크게 감소됨.

배치 정규화는 규제와 같은 역할을 하여 (이 장의 뒷부분에 나오는 드롭아웃 같은) 다른 규제 기법의 필요성을 줄여주게 한다.

**단점이라면,** 배치 정규화는 모델의 복잡도를 키우며, 더군다나 실행 시간 면에서도 손해이다. 층마다 추가되는 계산이 신경망의 예측을 느려지게 하기 때문에. 예측이 전광석화처럼 빨라야 한다면 배치 정규화를 사용하기 전에 ELU + He 초기화만으로 얼마나 잘 수행되는지 확인해보는 것이 좋습니다.

**//TODO 배치 정규화 구현해보기**



### 1-4 그래디언트 클리핑

역전파될 때 일정 임곗값을 넘어서지 못하게 그래디언트를 그냥 단순히 잘라내는 것- 이는 순환 신경망에서 일반적으로 널리 사용된다. 이를 **그래디언트 클리핑(Gradient Clipping)**. 

일반적으로 배치 정규화를 선호하지만 그래디언트 클리핑이 무엇인지 또 어떻게 구현하는지 알아두면 유용하다.



텐서플로에서 옵티마이저의 minimize() 함수는 그래디언트의 계산과 적용 두 가지를 모두 처리한다. 

그래서 이 함수를 대신하려면 옵티마이저의 compute_gradients() 메서드를 먼저 호출하고, 그다음에 clip_by_value() 함수를 사용해 그래디언트를 클리핑하는 연산을 생성합니다. 마지막으로 옵티마이저의 apply_gradients() 메서드를 사용해 클리핑된 그래디언트를 적용하는 연산을 만든다.

```python
threshold = 1.0

optimizer = tf.train.GradientDescentOptimizer(learning_rate)
grads_and_vars = optimizer.compute_gradients(loss)
capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)
              for grad, var in grads_and_vars]
training_op = optimizer.apply_gradients(capped_gvs)
```



## 2. 미리 훈련된 층 재사용하기

### 2-1 텐서플로 모델 재사용하기

일반적으로 아주 큰 규모의 DNN을 처음부터 새로 훈련시키는 것은 좋은 생각이 아니다. 해결하려는 것과 비슷한 유형의 문제를 처리한 신경망이 이미 있는지 찾아보고 그런 다음 그 신경망의 하위층을 재사용하는 것이 좋다.

이를 transfer learning 이라고 한다. 이 방법은 훈련 속도를 크게 높여줄 뿐만 아니라 필요한 훈련 데이터도 휠씬 적다. 

> TIP 만약 원래 문제에서 사용한 것과 크기가 다른 이미지를 입력으로 사용한다면 원본 모델에 맞는 크기로 변경하는 전처리 단계를 추가해야 한다. 일반적으로 전이 학습은 입력이 비슷한 저수준 특성을 가질 때 잘 작동한다.

![](https://ws2.sinaimg.cn/large/006tNc79gy1fzcak5m32nj30xq0kt0u5.jpg)

텐서플로우에서는 만들어져 있는 그래프에 접근하기 위해서는 import_meta_graph() 또는 Saver 객체를 사용해서 상태를 복원할 수 있다.

### 2-2 다른 프레임워크의 모델 재사용하기.

만약 모델이 다른 프레임워크로 훈련되어 있다면 수동으로 모델 파라미터를 읽어 들여 적절한 변수에 할당해야 합니다. 그러나 이는 매우 번거로운 작업이다.

```python
original_w = [[1., 2., 3.], [4., 5., 6.]] # 다른 프레임워크로부터 가중치를 로드
original_b = [7., 8., 9.]                 # 다른 프레임워크로부터 편향을 로드

X = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")
hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name="hidden1")
# [...] 모델의 나머지 부분을 구성

# hidden1 변수의 할당 노드에 대한 핸들을 구합니다
graph = tf.get_default_graph()
assign_kernel = graph.get_operation_by_name("hidden1/kernel/Assign")
assign_bias = graph.get_operation_by_name("hidden1/bias/Assign")
init_kernel = assign_kernel.inputs[1]
init_bias = assign_bias.inputs[1]

init = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init, feed_dict={init_kernel: original_w, init_bias: original_b})
    # [...] 새 작업에 모델을 훈련시킵니다
    print(hidden1.eval(feed_dict={X: [[10.0, 11.0]]}))  # 책에는 없음
```



### 2-3 신경망의 하위층을 학습에서 제외하기.

첫 번째 DNN의 하위층은 이미지에 있는 저수준 특성을 감지하도록 학습되어서 다른 이미지 분류 작업에 유용할 것 같습니다. 그러므로 이 층들은 그냥 있는 그대로 재사용할 수 있다. 

일반적으로 새로운 DNN을 훈련시킬 때 재사용되는 층들의 가중치를 '동결'하는 것이 좋다. 하위층의 가중치가 고정되면 (학습하려는 대상이 바뀌지 않기 때문에) 상위층의 가중치를 훈련시키기 쉽다. 훈련하는 동안 하위층을 고정시키는 한 가지 방법은 하위층의 변수를 제외하고 훈련시킬 변수 목록을 옵티마이저에 전달하는 것.

```python
reset_graph()

n_inputs = 28 * 28  # MNIST
n_hidden1 = 300 # 재사용
n_hidden2 = 50  # 재사용
n_hidden3 = 50  # 재사용
n_hidden4 = 20  # 새로 만듦!
n_outputs = 10  # 새로 만듦!

X = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")
y = tf.placeholder(tf.int32, shape=(None), name="y")

with tf.name_scope("dnn"):
    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name="hidden1")       # 재사용
    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name="hidden2") # 재사용
    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name="hidden3") # 재사용
    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name="hidden4") # 새로 만듦!
    logits = tf.layers.dense(hidden4, n_outputs, name="outputs")                         # 새로 만듦!

with tf.name_scope("loss"):
    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)
    loss = tf.reduce_mean(xentropy, name="loss")

with tf.name_scope("eval"):
    correct = tf.nn.in_top_k(logits, y, 1)
    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name="accuracy")
    
    
with tf.name_scope("train"):                                         # 책에는 없음
    optimizer = tf.train.GradientDescentOptimizer(learning_rate)     # 책에는 없음
    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,
                                   scope="hidden[34]|outputs")
    training_op = optimizer.minimize(loss, var_list=train_vars)
```

### 2-4 동결된 층 캐싱하기

동결된 층은 변하지 않기 때문에 각 훈련 샘플에 대해 가장 위쪽의 동결된 층에서 나온 출력을 캐싱하는 것은 가능하다. 전체 데이터셋에 대한 훈련이 여러 번 반복되기 때문에 훈련 샘플마다 동결된 층을 한 번만 거친다면(에포크마다 한 번씩이 아니라) 학습 속도를 크게 높일 수 있습니다. 예를 들어( 메모리가 충분하다고 가정하고) 전체 훈련 세트에 대해 하위층을 먼저 실행할 수 있다. 그리고 훈련하는 동안 훈련 샘플의 배치를 만드는 대신 은닉층 2의 출력을 배치로 만들어 훈련 연산에 주입.



### 2-5 상위층을 변경, 삭제, 대체하기.

원본 모델의 출력층은 새로운 작업에는 거의 쓸모가 없고 심지어 새 작업을 위한 출력 뉴런 수도 같지 않을 수 있기 때문에 보통 교체됩니다.

비슷하게 원본 모델의 상위층은 하위층보다는 덜 유용- 새로운 작업에서 필요한 고수준 특성은 원본 작업에서 유용했던 특성과는 많이 다르기 때문이다. 그래서 재사용할 적절한 층의 개수를 알아야 한다.

먼저 복사한 모든 층을 동결한다. 그다음에 모델을 훈련시키고 얼마나 성능이 나오는지 지겨봅니다. 그 후 가장 위쪽의 은닉층 한 개나 두 개의 동결을 해체해서 역전파로 가중치가 변경되게 하고 성능이 향상되는지 확인합니다. 훈련 데이터가 많을수록 많은 층을 동결 해제할 수 있다.

그래도 좋은 성능을 얻을 수 없고 훈련 데이터가 적다면, 가장 위쪽의 은닉층(들)을 제거하고남은 은닉층을 다시 모두 동결한다. 재사용에 적절한 층의 개수를 찾을 때까지 반복합니다. 훈련 데이터가 충분하다면 최상위 은닉층을 버리는 대신 바꿔볼 수 있으며, 심지어 은닉층을 더 추가할 수 있다.

### 2.6 모델 저장소

당명한 문제와 비슷한 작업을 훈련시킨 신경망을 어디에서 찾을 수 있나? 첫 번째로 찾아볼 곳은 자기 자신의 모델 카탈로그.

그러므로 자신의 모든 모델을 저장하고 나중에 쉽게 찾아볼 수 있도록 잘 정리해 놓아야 합니다. 다른 선택사항은 모델 저장소(model zoo)에서 찾아보는 것. 

카페 모델소도 있음

:https://github.com/BVLC/caffe/wiki/Model-Zoo#fully-convolutional-networks-for-semantic-segmentation-fcns

요걸 https://github.com/ethereon/caffe-tensorflow 활용해 Convert 시킨다.

### 2.7 비지도 사전 훈련

비지도 사전훈련(unsupervised pretraining) 할 수 있음 - 제한된 볼츠만 머신(Restricted Boltzmann Machines) 이나 **오토인코더**같은 비지도 특성 추출 알고리즘을 사용해 맨 하위층부터 위로 올라가면서 차례로 한 층씩 학습시킬 수 있다. 각 층은 훈련된 이전 층의 출력으로 훈련되며(훈련중인 층을 제외하고 다른 층은 모두 동결됩니다.) 이런 방식으로 모든 층이 훈련되면 지도 학습으로(즉, 역전파 알고리즘을 사용해) 신경망을 세밀하게 튜닝할 수 있다.

오늘날에는 일반적으로 RBM보다 오토인코더를 사용한 비지도 사전훈련이 더 좋은 선택이다.

![](https://ws1.sinaimg.cn/large/006tNc79gy1fzcbgbu2kaj30ye0nqq4r.jpg)

### 2-8 보조 작업으로 사전훈련

레이블된 훈련 데이터를 쉽게 얻거나 생성할 수 있는 보조 작업에 첫 번째 신경망을 훈련시키는 것. 그리고 이 신경망의 하위층을 실제 작업을 위해 재사용합니다 첫번째 신경망의 하위층은 두 번째 신경망에 재사용될 수 있는 특성 추출기를 학습하게 됩니다.

( 이해 안됨 )



# 3.고속 옵티마이저.

- 연결 가중치에 좋은 초기화 전략 적용하기.
- 좋은 활성화 함수 사용하기.
- 배치 정규화 사용하기.
- 미리 훈련된 신경망의 일부 재사용하기.
- 그리고...

훈련 속도를 크게 높일 수 있는 또 다른 방법으로 표준적인 경사 하강법 옵티마이저 대신 더 빠른 옵티마이저를 사용할 수 있다. 이 절에서는 가장 인기 있는 옵티마이저인 Momentum optimization, 네스테로프 가속 경사, AdaGrad, RMSProp, Adam 옵티마이저를 설명.


![](https://t1.daumcdn.net/cfile/tistory/996A04425AB85ED026)



![](https://ws2.sinaimg.cn/large/006tNc79gy1fzcx03rgjdj30t60deq5l.jpg)

- 가장 기초적인 학습식(Gradient Descent) 입니다. Error를 구할 때 전체 데이터를 고려해서 구하는 방법입니다 .전체 데이터를 고려해서 1회 학습했을 때 1epoch 이라고 부릅니다. 그런데 전체 데이터가 1억건 10억건이 되는 상황에서 전체를 고려해서 한 번씩 가중치를 수정하는 것은 시간이 오래 걸립니다.
- 그래서 1억건의 데이터라면 1천만 건씩 나누어서 학습을 진행하자. 는 아이디어를 통해서 나온 학습방법이 Stochastic Gradient Descent

![](https://ws1.sinaimg.cn/large/006tNc79gy1fzcx2xtn6mj30xu0h4gpw.jpg)

- Stochastic 방법을 사용해서 기존 방법보다 빨라지기는 했지만 만족할만한 수준은 아니었습니다. 또한 Cost의 최소점이 아닌 극소점을 찾은 뒤 더 이상 학습이 되지 않는 현상이 발생하기 시작했습니다.

![Optimizer](https://t1.daumcdn.net/cfile/tistory/9915A83E5AB8621703)

- 학습식을 보면 수정할 수 있는 부분이 Learning Rate와 Gradient 부분이라는 것을 알 수 있습니다. 그래서 Optimizer의 발전 과정은 어떤 부분을 수정하느냐에 따라 나누어진다.
- Gradient를 수정한 Momentum, Nag
- Learning Rate를 수정한 Adagrad, RMSProp, AdaDelta
- 이 두 종류의 장점을 합한 Adam, Nadam으로 구성한다.
- http://ruder.io/optimizing-gradient-descent/
- http://shuuki4.github.io/deep%20learning/2016/05/20/Gradient-Descent-Algorithm-Overview.html



### 3.1 Momentum

방향까지 고려한 것은 맞으나, 정확히는 관성을 이용한 것이라 보는 것이 맞음.

**그러므로, 이전의 그래디언트가 무엇이였는지 고려하는 것이 상당히 중요함.**

매 반복에서 현재 그래디언트를 (학습률 n을 곱한 후) **모멘텀 벡터** m 에 더하고 이 값을 빼는 방식으로 가중치를 갱신한다. 다시 말해 그래디언트를 속도가 아니라 가속도로 사용한다. 일종의 마찰저항을 표현하고 모멘텀이 너무 커지는 것을 막기 위해 이 알고리즘에는 모멘텀(momentum) 이라는 새로운 하이퍼파라미터 B가 등장한다. 이 값은 0과 1사이로 설정되어야 한다.


![](https://ws3.sinaimg.cn/large/006tNc79gy1fzcxe1hjvqj30z80oon0y.jpg)



### 3.2 Nesterov Accelerated Gradient (NAG)

Nesterov Accelerated Gradient(NAG)는 Momentum 방식을 기초로 한 방식이지만, Gradient를 계산하는 방식이 살짝 다르다. 

![zz](http://cs231n.github.io/assets/nn3/nesterov.jpeg)



Momentum 방식에서는 이동 벡터 $v_t$를 계산할 때 현재 위치에서의 Gradient와 Momentum step을 독립적으로 계산하고 합친다. 반면, NAG에서는 momentum step을 먼저 고려하여, momentum step을 먼저 이동했다고 생각한 후 그 자리에서의 gradient를 구해서 gradient step을 이동한다. 

참고 URL: https://tensorflow.blog/2017/03/22/momentum-nesterov-momentum/

기본 아이디어는 현재 위치가 아니라 모멘텀의 방향으로 조금 앞서서 비용 함수의 그래디언트를 계산하는 것.

![](https://ws3.sinaimg.cn/large/006tNc79gy1fzcxjuca7kj30vv0u0aey.jpg)



### 3.3 Adagrad(Adaptive Gradient)

한쪽이 길쭉한 그릇 문제를 다시 생각해보면, 경사 하강법은 가장 가파른 경사를 따라 빠르게 내려가기 시작해서 골짜기 아래로 느리게 이동합니다. 알고리즘이 이를 일찍 감지하고 전역 최적점 쪽으로 좀 더 정확한 방향을 잡았다면 좋았을 것입니다.

변수들을 update할 때 각각의 변수마다 step size를 다르게 설정해서 이동하는 방식이다. 이 알고리즘의 기본적인 아이디어는 '지금까지 많이 변화하지 않은 변수들은 step size를 크게 하고, 지금까지 많이 변화했던 변수들은 step size를 작게 하자'라는 것이다. 자주 등장하거나 변화를 많이 한 변수들의 경우 optimum에 가까이 있을 확률이 높기 때문에 작은 크기로 이동하면서 세밀한 값을 조정하고, 적게 변화한 변수들은 optimum 값에 도달하기 위해서는 많이 이동해야할 확률이 높기 때문에 먼저 빠르게 loss값을 줄이는 방향으로 이동하려는 방식이라고 생각할 수 있다.

특히, word2vec이나 Glove같이 word representation을 학습시킬 경우 단어의 등장 확률에 따라 variable의 사용 비율이 확연하게 차이나기 때문에 Adagrad와 같은 학습 방식을 이용하면 휠씬 더 좋은 성능을 거둘 수 있을 것이다.

AdaGrad는 가장 가파른 차원을 따라 그래디언트 백터의 스케일을 감소시켜 이 문제를 해결합니다.

![](https://ws3.sinaimg.cn/large/006tNc79gy1fzcxw6qu4ej30re0zk42m.jpg)

Adagrad를 사용하면 학습을 진행하면서 굳이 step size decay등을 신경써주지 않아도 된다는 장점이 있다. 보통 adagrad에서 step size로는 0.01 정도를 사용한 뒤, 그 이후로는 바꾸지 않는다. 반면, Adagrad에는 학습을 계속 진행하면 step size가 너무 줄어든다는 문제점이 있다. $G$에는 계속 제곱한 값을 넣어주기 때문에 $G$의 값들은 계속해서 증가하기 때문에, 학습이 오래 진행될 경우 step size가 너무 작아져서 결국 거의 움직이지 않게 된다. **이를 보완하여 고친 알고리즘이 RMSProp과 AdaDelta이다.**

### 3.4 RMSProp

AdaGrad는 너무 빠르게 느려져서 전역 최적점에 수렴하지 못하지만 RMSProp 알고리즘은 가장 최근 반복에서 비롯된 그래디언트만 누적함으로써 이 문제를 해결합니다.

Adagrad의 식에서 gradient의 제곱값을 더해나가면서 구한 $G_t$ 부분을 합이 아니라 지수평균으로 바꾸어서 대체한 방법이다. 이렇게 대체를 할 경우 Adagrad처럼 $G_t$가 무한정 커지지는 않으면서 최근 변화량의 변수간 상대적인 크기 차이는 유지할 수 있다. 식으로 나타내면 다음과 같다.

![](https://ws1.sinaimg.cn/large/006tNc79gy1fzcy0hbptkj30yz0k176q.jpg)

###  3.5 Adam (Adaptive Moment Estimation)

**적응적 모멘트 추정(Adaptive moment estimation)**을 의미하는 Adam은 모멘텀 최적화와 RMSProp의 아이디어를 합친 것.

모멘텀 최적화처럼 지난 그래디언트의 지수 감소 평균(Exponential decaying average)을 따르고 RMSProp처럼 지난 그래디언트 제곱의 지수 감소된 평균을 따른다.

![](https://ws1.sinaimg.cn/large/006tNc79gy1fzcy573namj30xc0u0q6f.jpg)

>  CAUTION_ 초기에 Adam이 무조건 좋다고 했지만, 2017년에 나온 한 논문에서 적응적인 최적화 방법이 일부 데이터셋에서 나쁜 결과를 만든다는 것을 보였다. 그러므로 지금은 모멘텀 최적화나 네스테로프 경사 가속을 사용하는 것이 나을 수있다.



### 3.6 학습률 스케줄링

이상적인 학습 곡선은 아래와 같음.

![](https://ws2.sinaimg.cn/large/006tNc79gy1fzccbgo3dlj31ui0tqk2m.jpg)

그렇다면 훈련하는 동안 학습률을 감소시키는 전략에는 여러 가지가 있는데, 이런 전략을 **학습 스케줄** 이라고 하며 가장 보편적인 것은 다음과 같다.

- **미리 정의된 개별적인 고정 학습률**  
  : 예를 들어 처음에 n=0.1로 학습률을 지정하고 50에포크 후에 n=0.001로 바꿉니다. 이 방법이 잘 작동할 수는 있지만 적절한 학습률과 적당한 시점을 찾으려면 이리저리 바뀌봐야 합니다.
- **성능 기반 스케줄링**  
  매 N 스텝마다(조기 종료처럼) 검증 오차를 측정하고 오차가 줄어들지 않으면 $\lambda$ 만큼 학습률을 감소시킵니다.
- **지수 기반 스케줄링**  
  반복 횟수 t의 함수 $\eta(t)$ = $\eta_010^-t/r$  로 학습률을 설정합니다. 이 방법이 잘 작동하지만 $\eta_0$ 와 $r$ 을 튜닝해야 합니다. 학습률은 매 r스탭마다 1/10씩 줄어들 것입니다.
- **거듭제곱 기반 스케줄링**  
  학습률을 $\eta(t)=\eta(1+t/r)^-c$ 으로 설정합니다. 하이퍼파라미터 c는 보통 1로 지정 지수 기반 스케줄링과 비슷하지만 학습률이 휠씬 느리게 감소된다.

성능기반 스케줄링과 지수 기반 스케줄링 둘 다 잘 작동했지만 구현과 튜닝이 쉽고 최적점에 조금 더 빨리 수렴하는 지수 기반 스케줄링이 선호된다고 결론을 내림.



# 4. 과대적합을 피하기 위한 규제 방법

"나는 네 개의 파라미터가 있으면 코끼리 모양을 학습시킬 수 있고, 다섯 개가 있으면 코를 꿈들거리게 할 수 있다." _존 폰 노이만

심층 신경망은 전형적으로 수만 개, 때로는 수백만 개의 파라미터를 가지고 있다.

**파라미터가 많기 때문에 네트워크의 자유도가 매우 높으며 크고 복잡한 데이터셋을 학습할 수 있다. 하지만 이런 높은 자유도는 훈련세트에 과대적합되기 쉽다는 것을 의미하기도 한다.**

**그러므로 규제해야 한다.** 여기서는 조기종료, L1와 L2 규제, 드롭아웃, 맥스-노름 규제, 데이터 증식을 설명한다.



### 4.1 조기 종료

훈련 세트에 과대적합되는 것을 피하기 위한 좋은 방법 하나는 조기졸업. 검증 세트의 성능이 떨어지기 시작할 때 훈련을 중지시키는 것.

### 4.2 L1와 L2 규제

![](https://ws3.sinaimg.cn/large/006tNc79gy1fzcys7qm5aj30yv0hetb3.jpg)

![](https://ws2.sinaimg.cn/large/006tNc79gy1fzcysfpjiuj30o40zk78w.jpg)

![](https://ws1.sinaimg.cn/large/006tNc79gy1fzcz2h8oboj30tg0f4aec.jpg)

참고 : http://taewan.kim/post/norm/#l1-norm

### 4.3 드롭아웃

![](https://ws4.sinaimg.cn/large/006tNc79gy1fzcyt21vvij30ya0nzdhs.jpg)

### 4.4 max_norm regularization

신경망에서 아주 널리 사용되는 또 다른 규제 기법은 **max-norm regularization**

이 방식은 각각의 뉴런에 대해 입력의 연결 가중치 w가 $||w||_2$ $\leq$ $r$ 이 되도록 제한하는 것. r은 맥스-노름 하이퍼파라미터이고 $||.||​$ 는 L2노름을 나타낸다.

일반적으로 매 훈련 스탭이 끝나면  $||w||_2$ 를 계산한 다음 w를 클리핑(?) 합니다. 

r을 줄이면 규제의 정도가 커져 과대적합을 감소시킵니다. 맥스-노름 규제는(배치 정규화를 사용하지 않았을 때) 그래디언트 감소/폭주 문제를 완화하는 데 도움을 줄 수 있습니다.

```python
def max_norm_regularizer(threshold, axes=1, name="max_norm",
                         collection="max_norm"):
    def max_norm(weights):
        clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)
        clip_weights = tf.assign(weights, clipped, name=name)
        tf.add_to_collection(collection, clip_weights)
        return None # 규제 손실을 위한 항이 없습니다
    return max_norm
```

```python
reset_graph()

n_inputs = 28 * 28
n_hidden1 = 300
n_hidden2 = 50
n_outputs = 10

learning_rate = 0.01
momentum = 0.9

X = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")
y = tf.placeholder(tf.int32, shape=(None), name="y")

max_norm_reg = max_norm_regularizer(threshold=1.0)

with tf.name_scope("dnn"):
    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,
                              kernel_regularizer=max_norm_reg, name="hidden1")
    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,
                              kernel_regularizer=max_norm_reg, name="hidden2")
    logits = tf.layers.dense(hidden2, n_outputs, name="outputs")
    
with tf.name_scope("loss"):
    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)
    loss = tf.reduce_mean(xentropy, name="loss")

with tf.name_scope("train"):
    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)
    training_op = optimizer.minimize(loss)    

with tf.name_scope("eval"):
    correct = tf.nn.in_top_k(logits, y, 1)
    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))

init = tf.global_variables_initializer()
saver = tf.train.Saver()
```

 ### 4.5 데이터 증식

>  NOTE 아주 깊은 심층 신경망을 훈련시킬 수 있는 또 다른 강력한 기법은 Skip connection을 추가하는 것이다.(스킵 커넥션은 한층의 입력을 상위층의 출력으로 보내는 것을 말합니다. )



# 5. 실용적 가이드 라인

**기본 DNN 설정**

초기화 - He 초기화

활성화 함수 - ELU

정규화 - 배치 정규화

규제 - 드롭아웃

옵티마이저 - 네스테로프 가속 경사

학습률 스케줄링 - 없음



다음과 같은 경우에는 위 기본 설정을 바꿀 필요가 있습니다.

- 좋은 학습률을 찾을 수 없다면 (수렴이 너무 느릴 때 학습 속도를 올리면 수렴은 빨라지지만 네트워크의 정확도는 최적화가 덜 되므로) 지수 감소 같은 학습 스케줄을 추가해볼 수 있다.
- 훈련 세트가 너무 작다면 데이터 증식을 수행할 수 있다.
- 희소 모델이 필요하면 L1규제를 추가할 수있다. (또는 훈련이 끝난 뒤 작은 가중치를 0으로 만듭니다.) 만약 더욱 희박한 희소 모델이 필요하면 네스테로프 가속 경사나 Adam 옵티마이저 대신 L1 규제와 함께 FTRL 알고리즘을 사용할 수 있다.
- 실행 속도가 아주 빠른 모델을 필요로 하면 배치 정규화를 빼고 ELU 활성화 함수를 LeakyReLU로 바꿀수 있다. 희소 모델을 만드는 것도 도움이 된다.



아쉬운 부분 Keras로 짠 것이 아니라서- 