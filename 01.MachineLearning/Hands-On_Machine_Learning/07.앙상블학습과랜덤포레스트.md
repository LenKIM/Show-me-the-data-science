# 앙상블 학습과 랜덤 포레스트

- 배깅
- 부스팅
- 스태킹

**대중의 지혜** - 무작위로 선택된 수천 명의 사람에게 복잡한 질문을 하고 대답을 모은다는 가정하에, 많은 경우 이렇게 모은 답이 전문가의 답보다 낫다.

## 1. 투표 기반 분류기

- 앙상블 학습의 가장 기초가 되는 것.

  : 바로 각 분류기의 예측을 모아서 가장 많이 선택된 클래스를 예측하는 것.



*= 직접 투표(hard voting) 투표기*

![z](https://t1.daumcdn.net/cfile/tistory/99835D385B5A56162B)

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

log_clf = LogisticRegression(solver='liblinear', random_state=42)
rnd_clf = RandomForestClassifier(n_estimators=10, random_state=42)
svm_clf = SVC(gamma='auto', random_state=42)

voting_clf = VotingClassifier(
    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],
    voting='hard')
voting_clf.fit(X_train, y_train)
```

```
VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='warn',
          n_jobs=None, penalty='l2', random_state=42, solver='liblinear',
          tol=0.0001, verbose=0, warm_start=False)), ('rf', Rando...f',
  max_iter=-1, probability=False, random_state=42, shrinking=True,
  tol=0.001, verbose=False))],
         flatten_transform=None, n_jobs=None, voting='hard', weights=None)
```

```python
from sklearn.metrics import accuracy_score

for clf in (log_clf, rnd_clf, svm_clf, voting_clf):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))
    
# LogisticRegression 0.864
# RandomForestClassifier 0.872
# SVC 0.888
# VotingClassifier 0.896
```

```python
log_clf = LogisticRegression(solver='liblinear', random_state=42)
rnd_clf = RandomForestClassifier(n_estimators=10, random_state=42)
svm_clf = SVC(gamma='auto', probability=True, random_state=42)

voting_clf = VotingClassifier(
    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],
    voting='soft')
voting_clf.fit(X_train, y_train)

# VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
#          intercept_scaling=1, max_iter=100, multi_class='warn',
#          n_jobs=None, penalty='l2', random_state=42, solver='liblinear',
#          tol=0.0001, verbose=0, warm_start=False)), ('rf', Rando...bf',
#  max_iter=-1, probability=True, random_state=42, shrinking=True,
#  tol=0.001, verbose=False))],
#        flatten_transform=None, n_jobs=None, voting='soft', weights=None)

from sklearn.metrics import accuracy_score

for clf in (log_clf, rnd_clf, svm_clf, voting_clf):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))
    

# LogisticRegression 0.864
# RandomForestClassifier 0.872
# SVC 0.888
# VotingClassifier 0.912

# https://github.com/keras-team/keras/issues/9672
```



# 2. 배깅과 페이스팅

같은 알고리즘을 사용하지만 훈련 세트에서 중복을 허용하여 샘플링하는 방식에 차이에 따라 아래와 같이 불린다.

##### 배깅(bagging) - bootstrap aggregation의 준말

훈련 세트에서 중복을 허용하여 샘플링하는 방식

##### 페이스팅(pasting)

중복을 허용하지 않고 샘플링하는 방식

![](https://cdn-images-1.medium.com/max/1600/0*zrm9Q8twgrq8lfLk.)

***일반적으로 앙상블의 결과는 원본 데이터셋으로 하나의 에측기를 훈련시킬때와 비교해 편향은 비슷하지만 분산은 줄어든다.***



사이킷런에는

```python
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

bag_clf = BaggingClassifier(
    DecisionTreeClassifier(), n_estimators=500, max_samples=100, bootstreap=True, n_jobs=-1)
bag_clf.fit(X_train, y_train)
y_pred = bag_clf.predict(X_test)
```

단일 결정 트리의 결정 경계와 500개의 트리를 사용한 배깅 앙상블의 결정 경계를 비교해보면

앙상블의 예측이 결정 트리 하나의 예측보다 일반화가 훨씬 잘된 것 같다.

*앙상블은 비슷한 편향에서 더 작은 분산을 만듭니다.(훈련 세트의 오차 수가 거의 비슷하지만 결정 경계는 덜 불규칙)*

![](https://ws3.sinaimg.cn/large/006tNc79gy1fzqv4cc03ej30ly07sjtc.jpg)

부트스트래핑은 각 예측기가 학습하는 서브셋에 다향성을 증가시키므로 배깅이 페이스팅보다 편향이 조금 더 높음

그래서 배깅이 페이스팅보다 더욱 선호되는 경향은 있지만, 여유가 된다면 둘 다 평가해서 더 나은 쪽을 선택.



### oob 평가(out-of-bag)

배깅이 중복을 허용해서 학습시키는 거라고 했다.  

이는 평균적으로 약 63%정도만 샘플링되고 나머지 37%를 oob샘플이라고 부른다.

***예측하는 동안 oob샘플은 사용하지 않으므로, 이 oob 샘플을 활용해 각 예측기의 oob평가를 평균해서 앙상블의 평가가 이루어진다.***

`oob_score = True` 

```python
bag_clf = BaggingClassifier(
    DecisionTreeClassifier(random_state=42), n_estimators=500,
    bootstrap=True, n_jobs=-1, oob_score=True, random_state=40)
bag_clf.fit(X_train, y_train)
bag_clf.oob_score_
# 0.912

bag_clf.oob_decision_function_ # oob 샘플에 대한 결정 함수의 값
```

---

## 3. 랜덤 패치와 랜덤 서브스페이스

BaggingClassifier는 특성 샘플링도 지원하는데, max_features, bootstrap_features 두 매개변수로 조절한다.

특히(이미지와 같은) 고차원의 데이터셋을 다룰 때 유용하다. 

훈련 특성과 샘플을 모두 샘플링하는 것은 **Random Patches method**

훈련 샘플을 모두 사용하고(즉, bootstrap=False이고 max_samples=1.0) 특성은 샘플링하는 (즉, bootstrap_features=True이고 max_features는 1.0보다 작은 ) 것은 **랜덤 서브스페이스 방식(Random Subspaces method)**

특성 샘플링은 더 다양한 예측기를 만들며 편향을 늘리는 대신 분산을 낮춤.

## 4. 랜덤 포레스트

랜던 포레스트는 일반적으로 배깅 방법을 적용한 결정 트리의 앙상블.

*즉, 훈련 세트에서 중복을 허용하여 샘플링 하는 방식을 활용했다.*

```python
from sklearn.ensemble import RandomForestClassifier

rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)
rnd_clf.fit(X_train, y_train)
# 최대 16개의 리프 노드를 갖는 500개 트리로 이뤄진 랜덤 포레스트
y_pred_rf = rnd_clf.predict(X_test)
```

랜덤 포레스트 알고리즘은 트리의 노드를 분할할 때 전체 특성 중에서 최선의 특성을 찾는 대신 무작위로 선택한 특성 후보 중에서 최적의 특성을 찾는 식으로 무작위성을 더 주입합니다. 이는 결국 트리를 다양하게 만들고 편향을 손해 보는 대신 분산을 낮추어 전체적으로 더 훌룡한 모델을 만들어 냅니다. 다음은 BaggingClassifier를 사용해 앞의 RandomForestClassifier

```python
bag_clf = BaggingClassifier(
	DicisionTreeClassifier(splitter="random", max_leaf_nodes=16),
    n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)
```

```python
bag_clf.fit(X_train, y_train)
y_pred = bag_clf.predict(X_test)
```

```python
from sklearn.ensemble import RandomForestClassifier

rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=42)
rnd_clf.fit(X_train, y_train)

y_pred_rf = rnd_clf.predict(X_test)
```

> np.sum(y_pred == y_pred_rf) / len(y_pred)  # 거의 동일한 예측

### 4-1 엑스트라 트리

랜덤 포레스트에서 트리를 만들 때 각 노드는 무작위로 특성의 서브셋을 만들어 분할에 사용. 더욱 무작위하게 만들기 위해서는 최적의 임계값을 찾는 대신 후보 특성을 사용해 무작위로 분할한 다음 그 중에서도 최상의 분할을 선택한다.

*일반적인 랜덤 포레스트는 모든 후보 특성의 최적의 임계값을 찾는 반면에, 엑스트라 트리는 후보 특성도 무작위로 선택해서 결정한다.*

이를, **익스트림 랜덤 트리(Extremely Randomized Trees)앙상블 또는 엑스트라 트리** 라고 부른다.

극단적인 분할을 활용

`ExtraTreesClassifier` 는 랜덤포레스트와 사용방식은 동일.



*무엇이 좋을지는 둘다 테스트를 해봐야 할 수 있다.*

### 4-2 특성 중요도

랜덤 포레스트의 또다른 장점은 특성의 상대적 중요도를 측정하기 쉽다라는 것. 

사이킷런은 어떤 특성을 사용한 노드가 (랜덤 포레스트에 있는 모든 트리에 걸쳐서) 평균적으로 불순도를 얼마나 감소시키는지 확인하여 특성의 중요도를 측정한다. 더 정확히 말하면 가중치 평균이며 각 노드의 가중치는 연관된 훈련 샘플 수와 같다.

사이킷런은 훈련이 끝난 뒤 특성마다 자동으로 이 점수를 계산하고 중요도의 전체 합이 1이 되도록 정규화하는데

이를 `feature_importances_` 변수에 저장한다.



## 5. 부스팅

**boosting**은 약한 학습기를 여러 개 연결하여 강한 학습기를 만드는 앙상블 방법을 말한다.(hypothesis boosting)이라고도 불림.

기본 아이디어는 앞의 모델을 보완해 나가면서 일련의 예측기를 학습시키는 것. 부스팅 방법에는 여러 가지가 있지만 

가장 인기 있는 것은 **AdaBoosting와 Gradient Boosting**이다.

### 5.1 AdaBoosting

*앞의 모델을 보완해 나가면서 일련의 예측기를 학습시키는 것* 의 의미는

![](https://ws2.sinaimg.cn/large/006tNc79gy1fzqw19jlw9j30nm079t9l.jpg)

```python
from sklearn.ensemble import AdaBoostClassifier

ada_clf = AdaBoostClassifier(
    DecisionTreeClassifier(max_depth=1), n_estimators=200,
    algorithm="SAMME.R", learning_rate=0.5, random_state=42)
ada_clf.fit(X_train, y_train)
```

 아다부스트 분류기를 만드려면 기반이 되는 첫 번째 분류기(예를 들면 결정 트리)를 훈련 세트에서 훈련시키고 예측을 만듭니다. 그다음에 잘못 분류된 훈련 샘플의 가중치를 상대적으로 높입니다. 두 번째 분류기는 업데이트된 가중치를 사용해 훈련 세트에서 훈련하고 다시 예측을 만듭니다. 그 다음에 다시 가중치를 업데이트하는 식으로 계속됩니다.

### 5.2 그래디언트 부스팅

아다부스트처럼 그래디언트 부스팅은 앙상블에 이전까지의 오차를 보정하도록 예측기를 순차적으로 추가하지만, 아다부스트처럼 반복마다 샘플의 가중치를 수정하는 대신 이전 예측기가 만든 잔여 오차(residual error)에 새로운 예측기를 학습시킵니다.

결정 트리를 기반으로 예측기를 만들면, **그래디언트 트리 부스팅** 또는 **그래디언트 부스팅 회귀 트리** 라고 한다.

![](https://ws3.sinaimg.cn/large/006tNc79gy1fzqwcamqcjj30lm0lsq64.jpg)

## 6. 스태킹(stacking)

'앙상블에 속한 모든 예측기의 예측을 취합하는 간단한 함수(직접 투표 같은)를 사용하는 대신 취합하는 모델을 훈련시킬 수 없을까요? ' 라는 기본 아이디어에서 출발했다고 함.

![](https://ws2.sinaimg.cn/large/006tNc79gy1fzqwgh7ey1j30hl0eimy8.jpg)

서로 다른 예측기는 각기 다른 값을 예측하고 마지막 예측기(**블렌더** blender 또는 **메타 학습기** )가 예측을 입력으로 받아 최종 예측을 만듭니다.

