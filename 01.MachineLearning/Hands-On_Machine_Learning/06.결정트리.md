# 결정 트리

*SVN처럼 **결정 트리(Decision Tree)** 분류와 회귀 작업 그리고 다중출력 작업도 가능한 다재다능한 머신러닝 알고리즘*



## Goal

- 결정 트리의 훈련
- 시각화
- 예측 방법

--

- 사이킷런의 CART 훈련 알고리즘
- 트리에 규제를 가하는 방법
- 희귀 문제에 적용하는 방법
- 결정 트리의 제약 사항



## 1. CART 훈련 알고리즘

 사이킷런은 결정 트리를 훈련시키기 위해(즉, 트리를 성장시키기 위해) CART(Classification And Regression Tree) 알고리즘 사용.

- 먼저 훈련 세트를 하나의 특성 k의 임곗값 $t_k$ 를 사용해 두 개의 서브셋으로 나눕니다. 어떻게 k와 $t_k$ 를 고를까?
- (크기에 따른 가중치가 적용된) 가장 순수한 서브셋으로 나눌 수 있는 (k, $t_k$ )짝을 찾습니다.

다음 아래의 알고리즘을 통해 최소화해야 하는 비용 함수는 다음과 같습니다.

![](https://ws1.sinaimg.cn/large/006tNc79gy1fzpo0yxpqnj30fc09kjsr.jpg)

- 훈련 세트를 성공적으로 둘로 나눈었다면 같은 방식으로 서브셋을 또 나누고 를 반복.
- 최대 깊이가 되면 중지하거나 불순도를 줄이는 분할을 찾을 수 없을 때 멈추게 됨.

> 즉, CART 훈련 알고리즘은 탐욕적 알고리즘(greedy algorithm) 또는 NP-완전 문제
>
> 그렇기 때문에 시간도 많이 필요하고 매우 작은 훈련 세트에는 적용하기 어렵다.

#### 그렇다면 계산 복잡도는?

예측을 하려면 결정 트리를 루트 노트에서부터 리프 노드까지 탐색해야 한다. 일반적으로 결정 트리는 거의 균형을 이루고 있으므로 결정 트리를 탐색하기 위해서는 약 O(log(m))개의 노드를 걸쳐야 한다. 그러므로 예측까지의 속도는 매우 빠르다.

그러나 훈련 알고리즘은 각 노드에서 모든 훈련 샘플의 모든 특성을 비교,그래서 훈련 복잡도는 O(n x mlog(m))이 됩니다.



## 2. 지니 불순도 또는 엔트로피?

기본적인 지니 불순도가 사용되지만 criterion 매개변수를  "entropy"로 지정하여 **엔트로피** 불순도를 사용할 수 있다.

Tip) DecisionTreeClassifier의 criterion 매개변수의 기본값은 'gini'이고, DecisionTreeRegressor의 기본값은 "mse"



**엔트로피?**

분자의 무질서함을 측정하는 것으로 원래 열역학의 개념입니다. 분자가 안정되고 질서 정연하면 엔트로피가 0에 가깝습니다. 



그럼 지니 불순도와 엔트로피 중 어떤 것을 사용해야 될까?

실제 크게 차이는 없고, 둘다 비슷한 트리를 만들어 냅니다. 지니 불순도가 조금 더 계산이 빠르기 때문에 기본값으로 좋습니다. 그러나 다른 트리가 만들어지는 경우 지니 불순도가 가장 빈도 높은 클래스를 한쪽 가지로 고립시키는 경향이 있는 반면, 엔트로피는 조금 더 균형 잡힌 트리를 만듭니다.

자세한 사항은 https://tensorflow.blog/2018/03/25/%EA%B2%B0%EC%A0%95-%ED%8A%B8%EB%A6%AC%EC%99%80-%EB%B6%88%EC%88%9C%EB%8F%84%EC%97%90-%EB%8C%80%ED%95%9C-%EA%B6%81%EA%B8%88%EC%A6%9D/ 참고



## 3. 규제 매개변수

#### 비파라미터 모델

결정 트리는 모델 파라미터가 전혀 없는 것이 아니라(보통 많습니다) 훈련되기 전에 파라미터 수가 결정되지 않았음

#### 파라미터 모델 

모델 구조가 데이터에 맞춰져서 고정되지 않고 자유롭습니다. 마치 선형 모델과 같음.

*과대적합을 줄이기 위한 규제는?*

DecisionTreeClassifier에서는

- min_samples_split(분할되기 위해 노드가 가져야 하는 최소 샘플 수)
- min_samples_leaf(리프 노드가 가지고 있어야 할 최소 샘플 수)
- min_weight_fraction_leaf(min_samples_leaf와 같지만 부여된 전체 샘플 수에서의 비율)
- max_leaf_nodes(리프 노드의 최대 수)
- max_features(각 노드에서 분할에 사용할 특성의 최대 수)

```python
from sklearn.datasets import make_moons
Xm, ym = make_moons(n_samples=100, noise=0.25, random_state=53)

deep_tree_clf1 = DecisionTreeClassifier(random_state=42)
deep_tree_clf2 = DecisionTreeClassifier(min_samples_leaf=4, random_state=42)
deep_tree_clf1.fit(Xm, ym)
deep_tree_clf2.fit(Xm, ym)

plt.figure(figsize=(11, 4))
plt.subplot(121)
plot_decision_boundary(deep_tree_clf1, Xm, ym, axes=[-1.5, 2.5, -1, 1.5], iris=False)
plt.title("규제 없음", fontsize=16)
plt.subplot(122)
plot_decision_boundary(deep_tree_clf2, Xm, ym, axes=[-1.5, 2.5, -1, 1.5], iris=False)
plt.title("min_samples_leaf = {}".format(deep_tree_clf2.min_samples_leaf), fontsize=14)

save_fig("min_samples_leaf_plot")
plt.show()
```

![](https://ws1.sinaimg.cn/large/006tNc79gy1fzpoqkffodj30ly07sdh1.jpg)

## 4. 불안정성

결정 트리가 장점이 많다는 것은 알겠지만, 책의 앞부분에 나온 것처럼 결정 트리는 결정 경계를 만듭니다(모든 분할은 축에 수직입니다.) 그래서 훈련 세트의 회전에 민감. 

![](https://ws1.sinaimg.cn/large/006tNc79gy1fzpp2zcwp3j30ls07sq3y.jpg)

간단한 선형으로 구분될 수 있는 데이터셋을 예로 들면, 왼쪽의 결정 트리는 쉽게 데이터셋을 구분하지만, 데이터셋을 45'회전한 오른쪽의 결정 트리는 불필요하게 구불구불해졌습니다. 두 결정 트리 모두 훈련 세트를 완벽하게 학습하지만 오른쪽 모델은 잘 일반화될 것 같지 않습니다. 이런 문제를 해결하는 한 가지 방법은 훈련 데이터를 더 좋은 방향으로 회전 시키는 PCA기법을 사용하는 것.

**특히, 사이킷런에서 사용하는 훈련 알고리즘은 확률적이기 때문에 같은 훈련 데이터에서도 다른 모델을 얻게 될 수 있다.**