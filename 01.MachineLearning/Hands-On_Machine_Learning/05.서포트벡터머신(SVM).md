# Support Vector Machine (SVM)



## 01. 선형 SVM 분류

SVM의 기본 아이디어는 선형적으로 구분되어 두 분류의 클래스가 직선으로 최대한 멀리 떨어져 있는 걸 표현.

##### : Large margin classification 이라함

도로 경계에 위치한 샘픔에 의해 전적으로 결정되는데, 이 때 샘플들을 서포트 벡터(Support Vector)라고 한다.

**c.f ) SVM은 특성의 스케일에 민감합니다. 특성의 스케일을 조정하면 결정 경계가 좋아진다.**  

```python
Xs = np.array([[1, 50], [5, 20], [3, 80], [5, 60]]).astype(np.float64)
ys = np.array([0, 0, 1, 1])
svm_clf = SVC(kernel="linear", C=100)
svm_clf.fit(Xs, ys)

plt.figure(figsize=(12,3.2))
plt.subplot(121)
plt.plot(Xs[:, 0][ys==1], Xs[:, 1][ys==1], "bo")
plt.plot(Xs[:, 0][ys==0], Xs[:, 1][ys==0], "ms")
plot_svc_decision_boundary(svm_clf, 0, 6)
plt.xlabel("$x_0$", fontsize=20)
plt.ylabel("$x_1$  ", fontsize=20, rotation=0)
plt.title("스케일 조정 전", fontsize=16)
plt.axis([0, 6, 0, 90])

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(Xs)
svm_clf.fit(X_scaled, ys)

plt.subplot(122)
plt.plot(X_scaled[:, 0][ys==1], X_scaled[:, 1][ys==1], "bo")
plt.plot(X_scaled[:, 0][ys==0], X_scaled[:, 1][ys==0], "ms")
plot_svc_decision_boundary(svm_clf, -2, 2)
plt.xlabel("$x_0$", fontsize=20)
plt.title("스케일 조정 후", fontsize=16)
plt.axis([-2, 2, -2, 2])

save_fig("sensitivity_to_feature_scales_plot")
```

![](https://ws2.sinaimg.cn/large/006tNc79gy1fz62l02ghkj30nb0563zg.jpg)

### 1-1. 소프트 마진 분류

모든 샘플이 도로 바깥쪽에 올바르게 분류되어 있다면 이를 **하드 마진 분류(Hard margin classification)**

그러나 2가지 문제

- 선형적으로 구분될 수 있어야 제대로 작동한다.

- 이상치에 민감하다.  

  ```python
  X_outliers = np.array([[3.4, 1.3], [3.2, 0.8]])
  y_outliers = np.array([0, 0])
  Xo1 = np.concatenate([X, X_outliers[:1]], axis=0)
  yo1 = np.concatenate([y, y_outliers[:1]], axis=0)
  Xo2 = np.concatenate([X, X_outliers[1:]], axis=0)
  yo2 = np.concatenate([y, y_outliers[1:]], axis=0)
  
  svm_clf2 = SVC(kernel="linear", C=10**9)
  svm_clf2.fit(Xo2, yo2)
  
  plt.figure(figsize=(12,2.7))
  
  plt.subplot(121)
  plt.plot(Xo1[:, 0][yo1==1], Xo1[:, 1][yo1==1], "bs")
  plt.plot(Xo1[:, 0][yo1==0], Xo1[:, 1][yo1==0], "yo")
  plt.text(0.3, 1.0, "불가능!", fontsize=24, color="red")
  plt.xlabel("꽃잎 길이", fontsize=14)
  plt.ylabel("꽃잎 너비", fontsize=14)
  plt.annotate("이상치",
               xy=(X_outliers[0][0], X_outliers[0][1]),
               xytext=(2.5, 1.7),
               ha="center",
               arrowprops=dict(facecolor='black', shrink=0.1),
               fontsize=16,
              )
  plt.axis([0, 5.5, 0, 2])
  
  plt.subplot(122)
  plt.plot(Xo2[:, 0][yo2==1], Xo2[:, 1][yo2==1], "bs")
  plt.plot(Xo2[:, 0][yo2==0], Xo2[:, 1][yo2==0], "yo")
  plot_svc_decision_boundary(svm_clf2, 0, 5.5)
  plt.xlabel("꽃잎 길이", fontsize=14)
  plt.annotate("이상치",
               xy=(X_outliers[1][0], X_outliers[1][1]),
               xytext=(3.2, 0.08),
               ha="center",
               arrowprops=dict(facecolor='black', shrink=0.1),
               fontsize=16,
              )
  plt.axis([0, 5.5, 0, 2])
  
  save_fig("sensitivity_to_outliers_plot")
  plt.show()
  ```

   그러므로, 이런 문제를 피하려면 좀 더 유연한 모델이 필요하다.  도로의 폭을 가능한 한 넓게 유지하는 것과 **마진 오류(Margin violation)** (즉, 샘플이 도로 중간이나 심지어 반대쪽에 있는 경우) 사이에 적절한 균형을 잡아야 한다. 이를 **소프트 마진 분류(Soft margin classification)** 

사이런킷의 SVM모델에서는 C 하이퍼파라미터를 사용해 이 균형을 조절할 수 있다.

- C값을 줄이면 도로의 폭이 넓어지지만 마진 오류도 커짐
- 반대로, C 값을 키우면 margin이 넓어졌지만 많은 샘플이 도로 안에 포함되어 있다. 그러나 두 번째 분류기가 더 잘 일반화될 것같아 보이지만, 사실 대부분의 마진 오류는 결정 경계를 기준으로 올바른 클래스로 분류되기 때문에 이 훈련 세트에서 예측 에러는 마진 오류보다 작다.

**Tip) SVM 모델이 과대적합이라면 C를 감소시켜 모델을 규제할 수 있다.**



**그리고 Hinge loss 함수가 등장하는데, hinge loss 란?**

## 02. 비선형 SVM 분류

비선형데이터를 다루는 한 가지 방법은

- 다항 특성과 같은 특성을 추가하는 것

이렇게 하면 선형적으로 구분되는 데이터넷이 만들어 질 수 있다.

```python
X1D = np.linspace(-4, 4, 9).reshape(-1, 1)
X2D = np.c_[X1D, X1D**2]
y = np.array([0, 0, 1, 1, 1, 1, 1, 0, 0])

plt.figure(figsize=(11, 4))

plt.subplot(121)
plt.grid(True, which='both')
plt.axhline(y=0, color='k')
plt.plot(X1D[:, 0][y==0], np.zeros(4), "bs")
plt.plot(X1D[:, 0][y==1], np.zeros(5), "g^")
plt.gca().get_yaxis().set_ticks([])
plt.xlabel(r"$x_1$", fontsize=20)
plt.axis([-4.5, 4.5, -0.2, 0.2])

plt.subplot(122)
plt.grid(True, which='both')
plt.axhline(y=0, color='k')
plt.axvline(x=0, color='k')
plt.plot(X2D[:, 0][y==0], X2D[:, 1][y==0], "bs")
plt.plot(X2D[:, 0][y==1], X2D[:, 1][y==1], "g^")
plt.xlabel(r"$x_1$", fontsize=20)
plt.ylabel(r"$x_2$", fontsize=20, rotation=0)
plt.gca().get_yaxis().set_ticks([0, 4, 8, 12, 16])
plt.plot([-4.5, 4.5], [6.5, 6.5], "r--", linewidth=3)
plt.axis([-4.5, 4.5, -1, 17])

plt.subplots_adjust(right=1)

save_fig("higher_dimensions_plot", tight_layout=False)
plt.show()
```

![](https://ws4.sinaimg.cn/large/006tNc79gy1fz62nkyrrvj30kk07ljre.jpg)

### 2-1 다항식 커널

다항식 특성을 추가하는 것은 간단하고 모든 머신러닝 알고리즘에서 잘 작동하지만 느림.

다행히도 SVM을 사용할 땐 **커널 트릭(Kernel trick)** 이라는 거의 기적에 가까운 수학적 기교를 적용.

```python
from sklearn.svm import SVC

poly_kernel_svm_clf = Pipeline([
        ("scaler", StandardScaler()),
        ("svm_clf", SVC(kernel="poly", degree=3, coef0=1, C=5))
    ])
poly_kernel_svm_clf.fit(X, y)
```

**과대적합 = 복잡도가 높음**

**과소적합 = 복잡도가 낮음**

TIP) 적절한 하이퍼파라미터를 찾는 일반적인 방법은 그리드 탐색을 사용하는 것입니다. 처음에는 그리드의 폭을 크게 하여 빠르게 검색하고, 그다음에는 최적의 값을 찾기 위해 그리드를 세밀하게 검색하는 것. 

```python
plt.figure(figsize=(11, 4))

plt.subplot(121)
plot_predictions(poly_kernel_svm_clf, [-1.5, 2.5, -1, 1.5])
plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])
plt.title(r"$d=3, r=1, C=5$", fontsize=18)

plt.subplot(122)
plot_predictions(poly100_kernel_svm_clf, [-1.5, 2.5, -1, 1.5])
plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])
plt.title(r"$d=10, r=100, C=5$", fontsize=18)

save_fig("moons_kernelized_polynomial_svc_plot")
plt.show()
```

![](https://ws4.sinaimg.cn/large/006tNc79gy1fz64e6ezi7j30m507xgm9.jpg)

### 2-2 유사도 특성 추가

비선형 특성을 다루는 또 다른 기법은 각 샘플이 특정 **랜드마크** 와 얼마나 닮았는지 측정하는 유사도 함수(Similarity function)로 계산한 특성을 추가하는 것. 예를 들어 앞에서 본 1차원 데이터셋에 두 개의 랜드마크 x1 = -2dhk x1= 1을 추가합니다.

```python
def gaussian_rbf(x, landmark, gamma):
    return np.exp(-gamma * np.linalg.norm(x - landmark, axis=1)**2)
```

**방사 기저 함수(Radial Basis Function)**

랜드마크의 크기를 선택하여 위 함수에 넣어 0(랜드마크에서 아주 멀리 떨어진 경우) 부터 1(랜드마크와 같은 위치일 경우)까지 변화하며 종 모양으로 나타낸다.

![](https://ws4.sinaimg.cn/large/006tNc79gy1fz64jdgsqoj30me07vjs3.jpg)

## 3. SVM 회귀

SVM을 회귀에 적용시키는 방법은 목표를 반대로 하는 것이다.  

일정한 마진 오류 안에서 두 클래스 간의 도로 폭이 가능한 한 최대가 되도록 하는 대신, SVM회귀는 제한된 마진 오류(즉, 도로 밖의 샘플) 안에서, 도로 안에 가능한 한 많은 샘플이 들어가도록 학습. 도로의 폭은 하이퍼파라미터 epsilon으로 조절한다.

![](https://ws3.sinaimg.cn/large/006tNc79gy1fz64qjcaonj30iv07wzl3.jpg)

마진 안에서는 훈련 샘플이 추가되어도 모델의 예측에는 영향이 없습니다. 그래서 이 모델을 **epsilon에 민감하지 않다.(e-insensitive)**고 말합니다.

사이킷런의 LinearSVR을 사용해 선형SVM 회귀를 적용해보자.

![](https://ws4.sinaimg.cn/large/006tNc79gy1fz650v3so2j30i807vab2.jpg)

NOTE) SVM을 이상치 탐지에도 사용할 수 있습니다.



## 4. SVM 이론

### 4-1. 결정 함수와 예측

선형 SVM 분류기 모델은 단순히 결정 함수를 계산해서 새로운 샘플 x의 클래스를 예측합니다. 결괏값이 0보다 크면 예측도니 클래스 y은 양성 클래스(1)가 됩니다. 그렇지 않으면 음성클래스(0)

### 4-2 목적 함수

결정 함수의 기울기를 생각해보면 이는 가중치 벡터 ||w||의 노름과 같습니다. 이 기울기를 2로 나누면 결정 함수의 값이 _+ 1이 되는 점들이 결정 경계로부터 2배 만큼 더 멀어집니다. 즉, 기울기를 2로 나누는 것은 마진에 2를 곱하는 것과 같습니다.

#### 작은 가중치 벡터가 라지 마진을 만듭니다

![](https://ws4.sinaimg.cn/large/006tNc79gy1fz65ak9d8ej30nl0680tc.jpg)

```python
def plot_2D_decision_function(w, b, ylabel=True, x1_lim=[-3, 3]):
    x1 = np.linspace(x1_lim[0], x1_lim[1], 200)
    y = w * x1 + b
    m = 1 / w
```

**하드 마진 선형 SVM 분류기의 목적 함수** 를 **제약이 있는 최적화(constrained optimization)** 문제로 표현할 수 있다.



---

# 연습문제

1. **서포트 벡터 머신의 근본 아이디어는 무엇인가요?**  
   클래스 사이에 가능한 한 가장 넓은 '도로'를 내는 것입니다. 다시 말해 두 클래스를 구분하는 결정 경계와 샘플 사이의 마진을 가능한 한 가장 크게 하는 것이 목적입니다. 소프트 마진 분류를 수행할 때는 SVM이 두 클래스를 완벽하게 나누는 것과 가장 넓은 도로를 만드는 것 사이에 절충안을 찾습니다.(즉, 몇 개의 샘플은 도로 안에 놓일 수 있습니다.) 또 하나의 핵심적인 아이디어는 비선형 데이터셋에서 훈련할 때 커널 함수를 사용하는 것입니다.
2. **서포트 벡터가 무엇인가요?**  
   SVM이 훈련된 후에 경계를 포함해 도로에 놓인 어떤 샘플입니다. 결정 경계는 전적으로 서포트 백터에 의해 결정됩니다. 서포트 백터가 아닌(즉, 도로 밖에 있는) 어떤 샘플도 영향을 주지 못합니다. 이런 샘플은 삭제하고 다른 샘플을 더 추가하거나, 다른 곳으로 이동시킬 수 있습니다. 샘플이 도로 밖에 있는 한 결정경계에 영향을 주지 못할 것입니다. 예측을 계산할 때는 전체 훈련 세트가 아니라 서포트 벡터만 관여됩니다.
3. **SVM을 사용할 때 입력값의 스케일이 왜 중요한가요?**  
   클래스 사이에 가능한 한 가장 큰 도로를 내는 것이므로 훈련 세트의 스케일이 맞지 않으면 크기가 작은 특성을 무시하는 경향이 있습니다.
4. **SVM 분류기가 샘플을 분류할 때 신뢰도 점수와 확률을 출력할 수 있나요?**  
   테스트 샘플과 결정 경계 사이의 거리를 출력할 수 있으므로 이를 신뢰도 점수로 사용할 수 있습니다. 그러나 이 점수를 클래스 확률의 추정값으로 바로 변환할 수는 없습니다. 사이킷런에서 SVM모델을 만들 때 probability=True로 설정하면 훈련이 끝난 후 (훈련 데이터에 5-겹 교차 검증을 사용하여 추가로 훈련시킨) SVM의 점수에 로지스틱 회귀를 훈련시켜 확률을 계산합니다. 이 설정은 SVM모델에 predict_proba()와 predict_log_proba() 메서드를 추가시킵니다.
5. **수백만 개의 샘플과 수백 개의 특성을 가진 훈련 세트에 SVM 모델을 훈련시키려면 원 문제와 쌍대 문제 중 어떤 것을 사용해야 될까요?**
6. **RBF커널을 사용해 SVM 분류기를 훈련시켰더니 훈련 세트에 과소적합된 것 같습니다. r(gamma)를 증가시켜야 할까요? 감소시켜야 할까요? C의 경우는 어떤가요?**
7. **이미 만들어진 QP알고리즘 라이브러리를 사용해 소프트 마진 선형 SVM 분류기를 학습 시키려면 QP 매개변수(H,f,A,b)를 어떻게 지정해야 하나요?**
8. **선형적으로 분리되는 데이터셋에 LinearSVC를 훈련시켜보세요. 그런 다음 같은 데이터셋에 SVC와 SGDClassifier를 적용해보세요. 거의 비슷한 모델이 만들어지는지 확인해보세요.**
9. **MNIST 데이터셋에 SVM 분류기를 훈련시켜보세요. SVM분류기는 이진 분류기라서 OvA전략을 사용해 10개의 숫자를 분류해야 합니다. 처리 속도를 높이기 위해 작은 검증 세트로 하이퍼파라미터를 조정하는 것이 좋습니다. 어느 정도까지 정확도를 높일 수 있나요?**
10. **캘리포니아 주택 가격 데이터셋에 SVM 회귀를 훈련시켜보세요.**