## 10. 연습문제

## 목차

~~10장. 인공 신경망 소개~~

~~10.1 생물학적 뉴런에서 인공 뉴런까지~~

~~10.2 텐서플로의 고수준 API로 다층 퍼셉트론 훈련하기~~

~~10.3 텐서플로의 저수준 API로 심층 신경망 훈련하기~~

~~10.4 신경망 하이퍼파라미터 튜닝하기~~

10.5 연습문제 



1. **초창기 인공 뉴런을 사용해 A +(XOR) B 를 계산하는 인공신경망을 그려보기.** 

   ![](https://ws3.sinaimg.cn/large/006tNbRwgy1fyoqm63lcyj30y40k4aez.jpg)

2. **고전적인 퍼셉트론(즉, 퍼셉트론 훈련 알고리즘으로 훈련된 단일 TLU) 보다 로직스틱 회귀 분류기가 일반적으로 선호하는 이유는 무엇인가요? 퍼셉트론을 어떻게 수정하면 로지스틱 회귀 분류기가 동등하게 만들 수 있나?**

   고전적인 퍼셉트론은 데이터셋이 전형적으로 구분될 때만 수렴하고 클래스 확률을 추정할 수 없습니다. 이와는 반대로 로지스틱 회귀 분류기는 데이터셋이 전형적으로 구분되지 못해도 좋은 솔루션으로 수렴하고 클래스 확률을 출력합니다. 퍼셉트론의 활성화 함수를 로지스틱 함수로(또는 여러 개의 뉴런일 경우 소프트맥스 활성화 함수로 ) 바꾸고, 경사 하강법을 사용하여(또는 크로스 엔트로피 같은 비용 함수를 최소화하는 다른 최적화 알고리즘을 사용하여) 훈련시키면 로지스틱 회귀 분류기와 동일하게 됩니다.

3. **왜 초창기의 다층 퍼셉트론을 훈련시킬 때 로지스틱 활성화 함수가 핵심 요소 였나?**  

   로지스틱 활성화 함수의 도함수는 어디에서나 0이 아니어서 경사 하강법이 항상 경사를 따라 이동할 수 있으므로 초창기 MLP의 핵심 요소 였습니다. 활성화 함수가 계단 함수일 때는 경사가 없기 때문에 경사 하강법이 이동할 수 없습니다.

4. **유명한 활성화 함수 네 가지는 무엇?**  

   계단 함수, 로지스틱, 하이퍼볼릭 탄젠트, ReLU

5. **10개의 통과 뉴런으로 된 입력층, 50개의 뉴런으로 된 은닉층, 그리고 3개의 뉴런으로 된 출력층으로 구성된 다층 퍼셉트론이 있다고 가정하자. 모든 뉴런은 ReLU 활성화 함수를 사용합니다.**

   - 입력 행렬 X의 크기는 얼마? m X 10 여기서 m은 훈련 배치의 크기

   - 은닉층의 가중치 백테 W(h) 와 편향 벡터 b(h)의 크기는 얼마? 10 X 50 , 50

   - 출력층의 가중치 백터W(o) 와 편향 벡터 b(o)의 크기는 얼마? 50 * 3, 3

   - 네트워크의 출력 행렬 Y의 크기는 얼마인가요? m * 3

   - X, W(h), b(h), W(o), b(o)의 함수로 네트워크의 출력 행렬 Y를 계산하는 식을 써보기.

     ![](https://ws1.sinaimg.cn/large/006tNbRwgy1fyoqtomt1mj30vq05eq4x.jpg)

6. 스펨 메일을 분류하기 위해서는 출력층에 몇 개의 뉴런이 필요할까? 출력층에 어떤 활성화 함수를 사용해야 될까? MNIST문제라면 출력층에 어떤 활성화 함수를 사용하고 뉴런은 몇 개가 필요할까? 1개, 확률을 추정해야 하므로 로지스틱 함수,

7. **역전파란 무엇이고 어떻게 작동하는가? 역전파와 후진 모드 자동 미분의 차이점은 무엇인가요?**

   // TODO

8. **다층 퍼셉트론에서 조정할 수 있는 하이퍼파라미터를 모두 나열해보세요. 훈련 데이터에 다층 퍼셉트론이 과대접합되었다면 이를 해결하기 위해 하이퍼파라미터를 어떻게 조정해야 할까요?**

   MLP에서 바꿀 수 있는 하이퍼파라미터는 은닉층 수, 각 은닉층의 뉴런 수, 각 은닉층과 출력층에서 사용하는 활성화 함수.
   만약 과대접합되었을 때는 은닉층 수와 각 은닉층에 있는 뉴런 수를 줄여볼 수 있다.

9. **깊은(deep) 다층 퍼셉트론을 MNIST 데이터셋에 훈련시키고 98% 정확도를 얻을 수 있는지 확인해보세요. 9장의 마지막 연습문제에서와 같이 모든 모든 부가 기능을 추가해보세요.(즉, 체크포인트를 저장하고, 중지되었을 때 마지막 체크포인트를 복원하고, 서머리를 추가하고 텐서보드를 사용해 학습 곡선을 그려보세요.)**