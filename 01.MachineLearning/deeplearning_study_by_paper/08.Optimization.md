# Optimization



### Gradient descent

Cost function을 최소화 하고싶은 것. 그래서 GD를 한다.



일반적으로 3가지 gradient가 있음.

1. 55.000 의 gradient를 각각 계산하고, 평균을 낸 뒤 업데이트를 하는 것을 Batch gradient descent

![](https://ws1.sinaimg.cn/large/006tNbRwgy1fwwyggmut5j30zo0fwwhi.jpg)

2. 한번에 한개씩만 업데이트 하는 것은 Stochastic gradient descent라고 한다.  

   **시간의 진행에 대해 확률적인 변화를 가지는 구조**

   ![](https://ws4.sinaimg.cn/large/006tNbRwgy1fwwyhl5wl2j31050pmdm3.jpg)

3. Mini-batch gradient decent

   ![](https://ws3.sinaimg.cn/large/006tNbRwgy1fwwyjbdin6j31190q1jzy.jpg)

   대부분 2의 n승으로 하는데 왜? 2의 n승일까?

   그러나 Mini-batch에도 문제가 있다.

   초기값을 잘못주면 학습이 안되고, 시간 제약이 있는 한계에서는 문제가 많다.

   ![](https://ws3.sinaimg.cn/large/006tNbRwgy1fwwym3u4xzj314b0lhjv4.jpg)



그래서 Gradient가 주어진때, Momentum이 도움이 된다. 한쪽방향으로는 값이 커지는데, 오실레이팅 한다.

![](https://ws4.sinaimg.cn/large/006tNbRwgy1fwwyngji8kj30w50ostdm.jpg)

![](https://ws2.sinaimg.cn/large/006tNbRwgy1fwwynuhr23j313p0pbq91.jpg)

![](https://ws2.sinaimg.cn/large/006tNbRwgy1fwwyocxv47j314r0l6jzl.jpg)

이게 좋은 이유가 무엇인가? 

이전의 정보를 활용해서 지금의 정보를 업데이트 하는 것





adagrad는?

만 개의 각각의 learning rate를 수정하고 싶을 때, 많이 변한 것은 조금 변하게 하고, 조금 변한 것은 많이 변하게 하는 것을 의미한다.



![](https://ws2.sinaimg.cn/large/006tNbRwgy1fwwys4n50cj312y0mfn4b.jpg)

**G의 의미는 every gradient 의 제곱을 의미한다.**

*"조금 변한 건 많이 / 많이 변한건 조금" 이라는 개념이 adagrad이다.*

그러나 치명적인 단점이 있는데, G가 계속 커질 경우, 러닝레이트가 계속 줄어들면서 해당 네트워크는 변하지 않을 것이다.

그래서 계속 줄어드는 것을 막기 위해 exponential moving average를 사용한다.

![](https://ws2.sinaimg.cn/large/006tNbRwgy1fwwz22njdqj31200ptgu4.jpg)

e를 더하는 이유는 0을 나누는 것을 막을려고.

**no learning late가 없다.**

RMSprop

![](https://ws2.sinaimg.cn/large/006tNbRwgy1fwwz4fihwrj30zh0mjdki.jpg)





에잇-담

모멘텀과 아담델타를 합친 것이 adam이다.

![](https://ws2.sinaimg.cn/large/006tNbRwgy1fwwz75z8ihj31270nqjym.jpg)

**e이 문제가 된다. cnn을 할 때는 e를 키우는 것이 좋다.**

현존하는 것을 최고.

![](https://ws4.sinaimg.cn/large/006tNbRwgy1fwwzd7h2j0j311f0o7guq.jpg)



그 외,

![](https://ws1.sinaimg.cn/large/006tNbRwgy1fwwzdznc3sj31430pdnbg.jpg)



쉬운 데이터를 먼저 학습하다가, 더 어려운걸 학습시키는것이 좋다.

![](https://ws4.sinaimg.cn/large/006tNbRwgy1fwwzeiva2aj30zx0mmdzh.jpg)

Batch nomalization / early stopping / gradient noise / Learning rate



![](https://ws2.sinaimg.cn/large/006tNbRwgy1fwwzfuqjydj30x10ofn4l.jpg)

**각각의 Learning rate의 특징을 확인하고 조절해야 한다!!!!!!**