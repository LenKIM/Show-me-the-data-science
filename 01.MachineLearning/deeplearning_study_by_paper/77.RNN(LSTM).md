## RNN(LSTM)

![](https://ws3.sinaimg.cn/large/006tNbRwgy1fys7eeeof8j31540u0n0x.jpg)



가장 큰 차이점은 Recurrent가 있다는 것!

이전에 추가된 정보를 같이 고려한다.

![](https://ws4.sinaimg.cn/large/006tNbRwgy1fys7ftmoq8j30kn0b0t9n.jpg)

RNN을 쓰는 이유는 시간적으로 corr 한 정보를 예측할 때 많이 사용합니다.

![](https://ws1.sinaimg.cn/large/006tNbRwgy1fys7hfgzfwj30kc0akgqf.jpg)

## Long Short term memory

어떤 입력이 들어오면 조합해서 아웃풋을 만들고, 그 다음 아웃풋을 만들고~

![](https://ws1.sinaimg.cn/large/006tNbRwgy1fys7hv7wsgj30l40bfab8.jpg)

단순히 이전의 정보와 현재의 정보를 취합을 해서 그 부분이 다음 NN에 들어간다.

훨씬 복잡해지는

![](https://ws4.sinaimg.cn/large/006tNbRwgy1fys7jmc810j30k70b4gn8.jpg)

Long Short Term

![](https://ws3.sinaimg.cn/large/006tNbRwgy1fys7kwtxhtj311p0oeqal.jpg)

Pointwise 각각의 dimentation에 맞게 곱하는 것



Cell state는 절대로 밖으로 빠져나가지 않는 것그냥 흘러감

Hidden State는 이전 출력을 의미함.



Forget Gate

Input Gate

Output Gate

위 3개의 게이트가 잘 조합되어 Long term / short term을 잘 분류하게 됨

![](https://ws1.sinaimg.cn/large/006tNbRwgy1fys7pb6uanj311i0oqgv0.jpg)

결국은 Cell state가 가장 중요하게 됨.

![](https://ws1.sinaimg.cn/large/006tNbRwgy1fys7q8j1efj310h0nctdt.jpg)

**결국은 게이트 메카니즘이 된다.**





각각의 dimention 

시그모이드 함수로 인해 0~1로 변화 된다.

![](https://ws4.sinaimg.cn/large/006tNbRwgy1fys7thssj2j30zq0ha7ck.jpg)

Forget게이트의 역할은

**Decide what information we're going to throw away from the cell state**

![](https://ws1.sinaimg.cn/large/006tNbRwgy1fys7vhzyptj30ya0bf432.jpg)

얼마나 반영할지(~C) 결정하게 된다.

Input Gate의 역할은

**Decide what new information we're going to store in the cell state**

![](https://ws2.sinaimg.cn/large/006tNbRwgy1fys7y4xwn3j30zh0o4aji.jpg)

![](https://ws3.sinaimg.cn/large/006tNbRwgy1fys7ypw6rsj312z0ohaid.jpg)

4개의 NN이 들어가 있음.

http://colah.github.io/posts/2015-08-Understanding-LSTMs/

