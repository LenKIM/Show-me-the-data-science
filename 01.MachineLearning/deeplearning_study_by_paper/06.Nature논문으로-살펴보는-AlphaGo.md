# Nature 논문으로 살펴보는 AlphaGo



DeepMind

게임으로 강화학습을 통해 전략을 찾아 문제를 해결함-

![](https://ws1.sinaimg.cn/large/006tNbRwgy1fwsg5fs95sj316o0mhqei.jpg)

![](https://ws2.sinaimg.cn/large/006tNbRwgy1fwsg5u7prcj30st0hhjuj.jpg)

`딥러닝이 중요하다`라고 보기 어려운 이유는? 

딥러닝에서 중요한 알고리즘은 MCT인데, 기존에 상관없어 보이는 논문에서 못하던 문제를 풀었음.



기존의 방법론에 딥러닝을 접합해서 문제를 풀었음.

19x19의 바둑판.

![](https://ws4.sinaimg.cn/large/006tNbRwgy1fwsga31a6nj313b0mqwyz.jpg)

![](https://ws1.sinaimg.cn/large/006tNbRwgy1fwsgcmxpz7j310t0oxdvf.jpg)

제일 그럴바 싶은 곳에 놓는 것. 즉, 도박의 뜻과 유사하다. 자세한 설명은 아래와 위키백과를 살펴보자.

http://www.rfdh.com/bas_sim/monte.htm

https://ko.wikipedia.org/wiki/몬테카를로_트리_탐색

4가지 상태로 계속 반복되는데,

Selection이란 Leaf 노드가

Expansion 늘리는거

![](https://ws2.sinaimg.cn/large/006tNbRwgy1fwsght3udwj311w0lf44t.jpg)

각각의 기법들에 딥러닝 기법들이 들어간다.

![](https://ws2.sinaimg.cn/large/006tNbRwgy1fwsgrkzh2wj313s0pndpx.jpg)

![](https://ws4.sinaimg.cn/large/006tNbRwgy1fwsgx92zglj313e0oqthx.jpg)

**Rollout policy**와 SL policy network의 경우에는

사람의 기보에서 학습한다.

input은 동일한데, 모양이 다르다.

같은 부분을 학습할 때는 **SL policy network**가 성능이 좋다.

그러나 **RL policy network** 의 경우에는 사람의 기보 + 같은 컴퓨터끼리 대결했을 때 나오는 기보

 **Value network** 판세를 평가하는 부분.



각각의 네트워크가 다 사용되는 것이 아니라, RL policy network는 Value network만 사용된다.


알파고 스스로의 기준으로 좋은지 나쁜지를 평가하는 판세- Value network

![](https://ws2.sinaimg.cn/large/006tNbRwgy1fwsgzr445uj310l0ppqh9.jpg)

![](https://ws2.sinaimg.cn/large/006tNbRwgy1fwsh01bitaj313d0o847p.jpg)

![](https://ws1.sinaimg.cn/large/006tNbRwgy1fwsh11oigpj311j0p27ge.jpg)

입력을 바둑판 자체가 아니라, 인간의 features을 가지고 활용.

![](https://ws1.sinaimg.cn/large/006tNbRwgy1fwsh212wwqj31270l8qf3.jpg)

![](https://ws4.sinaimg.cn/large/006tNbRwgy1fwsgx92zglj313e0oqthx.jpg)

![](https://ws4.sinaimg.cn/large/006tNbRwgy1fwsh5dwvuhj313r0pm486.jpg)

이긴판에는 리워드1

지는판에는 -1을 준다.

그래서 해당 기보에서 어떤 Action을 취하는데, 그 확률을 높이는 것(이긴것에 대해서!) 지는 것에 대해서는 반대로 낮추게 된다.

알파고랑 대결할 때 이길때마다 모두다 + 지는부분에 대해 -로 놓는다.

![](https://ws3.sinaimg.cn/large/006tNbRwgy1fwsh6ezyqbj311m0o5thp.jpg)

![](https://ws3.sinaimg.cn/large/006tNbRwgy1fwsh7b7i6mj31220opqdd.jpg)

![](https://ws1.sinaimg.cn/large/006tNbRwgy1fwsh7u52mlj30yx0oetid.jpg)



![](https://ws3.sinaimg.cn/large/006tNbRwgy1fwsha4lwdoj311j0ng0yt.jpg)

edge에는 2가지 중요한 정보가 있는데, the number of evaluation / action values

rollout net은 끝까지 다 돌렸을 때 마지막에 남은 스코어 



![](https://ws1.sinaimg.cn/large/006tNbRwgy1fwshcyicegj31290modmj.jpg)

![](https://ws2.sinaimg.cn/large/006tNbRwgy1fwshe1rs5wj31360phk1w.jpg)

