## Regularization 이란?

Main purpose is to avoid **"Overfitting"**

=> 학습을 너무 믿는 것...

![](https://ws1.sinaimg.cn/large/006tNbRwgy1fwq6hw8jcnj312p0ovdzg.jpg)



#### 학습데이터를 다 맞춘다고 해서 좋은 것이 아니다.

#### 학습 데이터를 너무 믿는 것이 Overfitting 이라고 한다.

![](https://ws4.sinaimg.cn/large/006tNbRwgy1fwq6jimcs3j314f0qc7mz.jpg)

under-line function이 어떻게 생겼는데, 모른다.

문제가 되는 것은 테스트커질때, Train data가 줄어들었다 하면 문제가 된다.

**predictor => Train data 와 Test data의 차이를 말한다.**

![](https://ws2.sinaimg.cn/large/006tNbRwgy1fwq6n4xzuvj30zx0qkq7w.jpg)

![](https://ws4.sinaimg.cn/large/006tNbRwgy1fwq6nm31ruj30x70pgdm9.jpg)



![](https://ws1.sinaimg.cn/large/006tNbRwgy1fwq6p067gvj316m0pge81.jpg)

![](https://ws4.sinaimg.cn/large/006tNbRwgy1fwq6pddj35j316h0pwkht.jpg)

![](https://ws2.sinaimg.cn/large/006tNbRwgy1fwq6qfvc8lj312w0qqagh.jpg)

Generalization performance가 높아지는 것이 목표이다.

만약 점점 둘 사이의 거리가 멀어지면 Over Fitting이 날 확률이 커진다.

## 오버피팅을 막기 위해서는....

3.앙상블은 정말 범용적으로 사용되기 때문에- Kaggle에서 항상 앙상블은 먼저 쓰고 접근 

4 - 테크니컬한 방법을 시도한다.

![](https://ws4.sinaimg.cn/large/006tNbRwgy1fwq6tzbtx1j314x0r11it.jpg)

![](https://ws2.sinaimg.cn/large/006tNbRwgy1fwq6uz2s3cj312t0qodtb.jpg)

![](https://ws1.sinaimg.cn/large/006tNbRwgy1fwq6ysiw4dj312o0pctb0.jpg)

위 공식을 따름. 줄인다.



![](https://ws1.sinaimg.cn/large/006tNbRwgy1fwq6zcrhysj312v0nvk1m.jpg)

![](https://ws4.sinaimg.cn/large/006tNbRwgy1fwq704t6zvj318b0oa13n.jpg)

![](https://ws3.sinaimg.cn/large/006tNbRwgy1fwq7248h6bj31620r4aj1.jpg)



### 데이터가 적을 때는 각각의 하이퍼파라미터가 정말 중요하고, 직접 코딩해서 해봐야 한다.

### 아트의 영역이다- 감이 중요하다-



http://www.deeplearningbook.org

![](https://ws3.sinaimg.cn/large/006tNbRwgy1fwq75am56vj312i0qtgv4.jpg)

![](https://ws1.sinaimg.cn/large/006tNbRwgy1fwq793al7aj31110o47ct.jpg)

**제곱을 더하거나 / 절대값을 더하는 방식으로 파라미터에 대한 퍼널티를 준다.**

![](https://ws1.sinaimg.cn/large/006tNbRwgy1fwq7arf5otj31300oxgz5.jpg)

![ ](https://ws4.sinaimg.cn/large/006tNbRwgy1fwq7cur7zlj313y0puwq4.jpg)

input과 output이 다 있을 때 supervised 

input만 있는 것은 unsupervised

![](https://ws4.sinaimg.cn/large/006tNbRwgy1fwq813xpqvj313c0jw49r.jpg)

멀티 테스킹 러닝이란? 한번에 문제를 푸는 것

이것이 왜 잘되는가? Shared 부분의 구조가 핵심이다.

![](https://ws3.sinaimg.cn/large/006tNbRwgy1fwq82wja1fj314w0qeqge.jpg)

 ![](https://ws2.sinaimg.cn/large/006tNbRwgy1fwq8542mxuj314m0o5gww.jpg)

![](https://ws1.sinaimg.cn/large/006tNbRwgy1fwq86ugepsj31380pbqd9.jpg)

0을 중간에 만드는 것이다.

활성화 함수 중 Relu가 위 Sparse activations의 주요 예시가 될 수 있다.





![](https://ws2.sinaimg.cn/large/006tNbRwgy1fwq89lhx40j316y0pm7lh.jpg)

Bagging은 나온 값들을 평균을 만들어서 내면 성능이 높아진다.

Boosting은 아다부스트 



학습시킬때 noise를 조금 넣어 일부변화를 방지한다.

![](https://ws3.sinaimg.cn/large/006tNbRwgy1fwq8bw02fbj31310ou177.jpg)

adversarial examples가 존재하다는 의미는, 기울기가 굉장히 가파르다. 이런건 좋지 않다. 때문에 이러한 adversarial을 방지하기 위해 트레이닝 시켜야 한다.