1. Ordinary least squares 최소제곱법 선형회귀

2. Mean-squared-error 평균제곱오차

3. coefficient 계수

4. intercept  절편

5. Ridge 선형회귀와 유사하지만- 가중치 선택은 훈련데이터 뿐만아니라 추가제약 조건을 만족시키 위한 목적도 있다. 여기서 추가제약 조건이란? regularization이란 과대적합이 되지 않도록 모델을 강제로 제한한다.  L2규제

   *최소제곱법을 사용하는 선형회귀야 다른 부분이 무엇인지 인지했는가?*

6. 선형 회귀에 규제를 적용하는데 Ridge의 대안으로 Lasso가 있음. 릿지회귀에서와 같이 Lasso도 계수를 0에 가깝게 만들려고함. 그러나, 방식이 조금 다른데 어떻게? L1 규제라고 하는데, 이는 어떤 계수를 정말 0으로 만들어 버린다는 의미이다. 



---

## #분류용 선형 모델

**Binary classification** - **결정 경계**가 입력의 선형 함수

- 특정 계수(Coefficient)와 절편(Offset)의 조합이 훈련 데이터에 얼마나 잘 맞는지 측정하는 방법
- 사용할 수 있는 Regularization가 있는지, 있다면 어떤 방식인지?

가장 널리 알려진 두 개의 선형 분류 알고리즘은 linear_model.LogisticRegression(로지스틱 회귀)에 구현된 Logistic regression과 svm.LinearSVC(SVC는 support vector classifier) (선형 서포트 벡터 머신)  // linear_model.LogisticRegression 회귀라는 단어가 들어가지만, 회귀알고리즘이 아니라 분류 알고리즘이다. 

LogisticRegression에서 Loss Function을 활용하고 다중 분류에서는 Cross-entropy(교차 엔트로피) 손실함수를 사용합니다.

svm.LinearSVC 클래스의 기본 값은 squared hinge 손실 함수를 사용.

**Multiple Classification** - 다중 클래스 분류용 선형 모델

대부분의 이진분류 알고리즘이 다중 클래스 분류용으로 확장되는 보편적인 기법은 일대다 one vs -rest 방법. 일대다 방식은 각 클래스를 다른 모든 클래스와 구분하도록 이진 분류 모델을 학습시킨다.

Regularization의 정도가 L1이나 L2이냐에 따라, 어떻게 사용할지 정해야 한다. 중요한 특성이 많지 않다고 생각하면 L1규제를 사용하고, 그렇지 않다면 기본적으로 L2규제를 사용해야 한다.

선형 모델은 학습 속도가 빠르고 예측이 빠르다. 매우 큰 데이터셋과 희소한 데이터셋에도 잘 동작한다. 수십만에서 수백만 개의 샘플로 이뤄진 대용량 데이터셋이라면 기본 설정보다 빨리 처리하도록 LogisticRegression과 Ridge에 solver='sag 옵션'을 줍니다. 다른 대안으로 SGDClassifier와 SGDRegressor를 사용한다.

##  # 나이브베이즈분류기

각 특성을 개별로 취급해 파라미터를 학습하고 각 특성에서 클래스별 통계를 단순하게 취합하기 때문



## # 결정 트리(decision tree)

분류와 회귀 문제에 널리 사용되는 모델. 결정 트리를 학습한다는 것은 정답에 가장 빨리 도달하는  예/아니오 질문 목록을 학습한다는 뜻. 여기서 질문이 테스트

![](https://ws1.sinaimg.cn/large/006tNbRwgy1fwtise34snj31kw0kl0yy.jpg)

결정 트리에서 모델 복잡도를 조절하는 매개변수는 트리가 완전히 만들어지기 전에 멈추게 하는 사전 가지치기 매개변수. 사전 가지치기 방법 중 max_depth, max_leaf_nodes또는 min_samples_leaf중 하나만 지정해도 과대적합을 막는 데 충분.

결정 트리의 장점은

첫째, **만들어진 모델을 쉽게 시각화할 수 있어서 비전문가도 이해하기 쉽다. 그리고 데이터의 스케일에 구애받지 않는다.** 각 특성이 개별적으로 처리되어 데이터를 분할하는데 데이터 스케일의 영향을 받지 않으므로 결정 트리에서는 특성의 정규화나 표준화 같은 전처리 과정이 필요 없습니다. 특히 특성의 스케일이 서로 다르거나 이진 특성과 연속적인 특성이 혼합되어 있을 때도 잘 동작합니다. 단점은 사전 가지치기를 사용함에도 불구하고 과대접합되는 경향이 있어 일반화 성능이 좋지 못하다.

## # 결정 트리의 앙상블

**앙상블? - > 여러 머신러닝 모델을 연결하여 더 강력한 모델을 만드는 기법. 그 중에서 가장 효과적이다 싶은 모델은 2가지**

랜덤 포레스트(random forest)와 그래디언트 부스팅(gradient boosting)결정 트리. 둘 다 모델을 구성하는 기본 요소.

#### 랜덤 포레스트

훈련 데이터에 과대적합되는 경향을 회피할 수 있는 방법으로서, 조금씩 다른 여러 결정 트리의 묶음.

랜덤 포레스트에서 트리를 랜덤하게 만드는 방법은 두 가지.

1. 트리를 만들 때 사용하는 데이터 포인트를 무작위로 선택하는 방법
2. 분할 테스트에서 특성을 무작위로 선택하는 방법



![](https://ws2.sinaimg.cn/large/006tNbRwgy1fwtm2ki5p6j30qd0ohwm8.jpg)



왜 랜덤포레스트?

기본적으로 랜덤포레스트는 단일 트리의 단점을 보완하고 장점은 그대로 가지고 있다. 여기서 단일 트리의 단점이란 과소적합을 의미 . 만약 의사 결정 과정을 간소하게 표현해야 한다면 단일 트리를 사용할 수 있다. 왜냐하면 수십, 수백 개의 트리를 자세히 분석하기 어렵고 랜덤 포레스트의 트리는 (특성의 일부만 사용하므로) 결정 트리보다 더 깊어지는 경향도 있기 때문이다. 대량의 데이터셋에서 랜덤 포레스트 모델을 만들 때 다소 시간이 걸릴 수 있지만 CPU코어가 많다면 손쉅게 병렬 처리할 수 있음.

즉, 하드웨어 사양을 많이 탄다.

#### 그래디언트 부스팅 회귀 트리

여러 개의 결정 트리를 묶어 강력한 모델을 만드는 또 다른 앙상블 방법

랜덤 포레스트보다는 매개변수 설정에 조금 더 민감하지만 잘 조정하면 더 높은 정확도를 제공한다. 그렇기 때문에 learning_rate가 중요하며, 트리는 보정을 강하게 하기 때문에 복잡한 모델을 만듭니다. n_estimatiors 값을 키우면 앙상블에 트리가 더 많이 추가되어 모델의 복잡도가 커지고 훈련 세트에서의 실수를 바로잡을 기회가 더 많아진다.